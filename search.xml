<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>纪念我的学生生涯</title>
      <link href="/Commemorate_my_student_career.html"/>
      <url>/Commemorate_my_student_career.html</url>
      
        <content type="html"><![CDATA[<p>2000-2020，我在学校度过了21世纪的前20年。即将踏上工作的征程，回首过去的20载，我感触多多…</p><a id="more"></a><ul><li><p>2000-2002</p><p>幼儿园时光，只记得从我姥姥家回家上学了，告别了天天玩耍的日子，去学校认识了很多小伙伴，至此开启了我的学生生涯。</p></li><li><p>2002-2007</p><p>小学时光，小学是漫长而又短暂的。小的时候一直希望长大，总觉得时间过得很漫长，迟迟才毕业。现在回想起来那时候是多么的悠然，现在的我又何尝不想回到那时候。</p><p>记得那时候天天只知道玩，不知道学习，出去一逛就是一天，每天晚上都是顶着天黑拖着脏兮兮的躯体回去吃饭，家长也不说啥，现在想想那样的童年是多么美好。</p><p>二年级以前是很羡慕别的同学能领到奖状的，可是咱太贪玩，奖状这种事跟咱也无缘。还记得一年级第一次考试我还得意的提前交卷，可成绩出来之后也没有得满分，这样几次下来之后我就再也不提前交卷了。做完了就检查，一遍又一遍。</p><p>一二年级经常被老师骂，可能是咱太调皮？或许很多被老师夸的场景记不太清，记得那时候就是被骂过来的。有次在期末前复习我课堂上回答问题还说错了，老师就说你这次肯定一个奖状也拿不了。或许当时把我幼小的心灵深深刺痛了，所以我当时回去努力学习了？忘记了。反正那次期末考试我竟然破天荒的拿了我人生第一个奖状。之后就每年都会领到奖状，咱也从此跻身全班&quot;优秀学生&quot;行列了😄。</p><p>无奈班上的几个同学实力太强，我也一直保持在全班前五的位置（总共30多个同学）。最好的一次是三年级上学期，我拿了语文三等奖，数学三等奖，总分三等奖的好成绩，那是我小学的巅峰时刻。还记得当时发了一支钢笔，第二天带到学校就丢了😭。</p><p>再然后就是小学毕业，班里的同学也各奔东西，去了不同的地方去上初中。当时就觉得终于要离开家了，长这么大了还哪里都没去过呢，很是兴奋。当时也没有过那种同学分别依依不舍的情形，就只是兴奋与迷茫。我妈当时让我跟我班学习成绩最好的一位同学去县里的二中，现在想想我妈也是明智的很。</p></li><li><p>2007-2010</p><p>初中跟小学相比那真是一点都不一样，刚去的时候还因为口音问题被同学嘲笑😡。而且一个班有七八十号人，身高分布也参差不齐，大的欺负小的，地头蛇欺负离得远的。总之太多的校园暴力事件在当时太正常不过了。有的人还谈起了恋爱，咱还挺羡慕。记得初一的时候班里有个女同学写了个纸条让人给我传过来，到我手里的时候大家都抢的看，我心一慌就把那纸条撕了，现在想想挺对不起人家的。</p><p>混社会吧，咱还不太行，同情心太重，打人下不了手。学习成绩呢也一般般，能排个班里前十名，每天晃晃悠悠的就到了初三。</p><p>在讲初三之前我要好好感谢一下我的初三班主任，也是我的语文老师。因为我们初二升学考试的成绩决定了初三的分班，我那时被分到了普通班。进班的时候班主任就说以我的成绩好好学个一年能考上文中的（我们县最好的高中，全县的考生梦寐以求的地方）。当时我对文中映像都不深刻，而且感觉就像天上人间似的遥不可及。我知道班主任当时也是可能安慰我，给我打气，但我当时的信心突然就上来了。</p><p>我们宿舍总共有12个人，每个人学习都特别努力，这些人也组成了我们班的&quot;优秀天团&quot;。每次考试前几名都是我们宿舍的，我刚开始考试也是徘徊在前4名左右，经过一年的努力学习，竟有时候能拿个第一名。那可是我人生中的第一个第一名，激动了我 好久。最后冲刺阶段的几次考试我稳定在了班里第二名的位置，他们1,3,4名来回换，我竟纹丝不动，班主任都夸我学习很稳😜。最后中考我考了第四，想想原因，可能是考试太多，麻木了，么有一点紧张的样子。就这样我成功考上了梦寐以求的文水中学。</p><p>毕业了，初中的小伙伴又各奔东西了，有的和我一样上了文中，有的没考上去了其他高中，有的干脆不上学了。那时候的我就有了一种五味杂陈的感觉，就觉得人在每个十字路口的选择可能都不同，不同的选择将会锻造出不同的人生。</p></li><li><p>2010-2013</p><p>也不知道听谁说的，高中的学习很轻松，而且刚去的时候身边的同学也都是这样的。我因为一开始的军训教官没让我参加汇演，肚里憋了一肚子火，将这股火转化为动力，所以一开始学的贼认真。刚进班的时候按中考成绩我排第四，第一次考试我就窜到了第一，至此我整个高中生涯都是在第一的位置，很稳😝。</p><p>县里最好的高中就是不一样啊，这学生素质一下就上来了。校园里少了很多杀戮，多了很多谈论学习的情形。那时我也才接触电脑不久，哎，沉迷了网络游戏。跟着一个班经常出去上网的同学，有时候还通宵，第二天早上从网吧出来的那种感觉就跟刚从监狱出来似的。但是就算这样也没有太影响了学习，因为游戏玩的也菜😂，就可能没有那么的迷恋吧。</p><p>高二分文理我又进了另一个班，在这个班我找到了现在的女朋友。一路上坎坎坷坷，分分合合一直到了高考。高三的生活是无聊的，这种生活跟我当时初三的感觉是完全不同的。现在想想可能跟第一名的压力有很大关系。以至于到了大学我竟不想去追逐最好，不想在成绩上去竞争第一名。</p><p>高三毕业我是感慨良多的，那么多的好朋友就将分别，而这种分别是天南海北的。当然也是恋人的分别，天各一方，很难相见，但终是依依不舍。</p></li><li><p>2013-2017</p><p>大学生活是令人向往的，咱也是个大学生了。大学的学习跟以前看起来是完全不同的，多了很多自由，没有人管你，全靠自觉。但现在回过头来看，你完全可以按照高中的学习模式去上大学，就天天看书，天天做题呗。先把课本上的知识弄懂再说。理论知识对后面的进一步提升自己很重要。当然了，大学生活增加了很多课外的实践，可以参加很多比赛。我在大三的时候进了学院的智能车实验室，从一开始懵懂无知，自己努力学习，慢慢积累，到后面代表学校拿到华东赛区一等奖，全国二等奖，也算很欣慰了。学会了很多，但不会的感觉好像更多了😂。</p><p>临近毕业，自己是工作还是读研，我毅然决然的选择了后者。因为当时对我工作是迷茫的，不知道该干啥，也不知道自己能干啥，考研继续提升一下自己。现在想想这个决定相当正确。</p><p>因为我参加完智能车比赛已经是8月底了，那时候准备考研确实有点仓促。这里多谢当时陪我一块考研的小伙伴，是你们在我一开始困难的时候帮助了我，平时有什么不会的问题的时候你们也很耐心的教我。经过几个月的努力，我考上了，而且是高分，407，真是个惊喜，又惊又喜。</p><p>大学毕业了，同学又各自散去，这次的分别有可能就是一辈子的分别，可能一辈子也不会再见了，伤感的很。但当时的我还沉浸在研究生入门的新奇之中。</p></li><li><p>2017-2020</p><p>研究生了，学历越来越高 ，学习也越来越认真了，这时候的学习真正是主动学习。为了自己的未来，为了自己的前途，拼命的学习。跟着导师做了医学图像，在这一领域想做出一些成就。关于读博，一开始想的也是如果研究生期间能够发表一定数量及质量的相关论文，那么自己就应该适合继续搞科研，要不读博反而是国家的损失了。如果发现自己不适合，就努力锤炼自己的工作技能，为3年后的找工作做准备。</p><p>3年过去了，我工作了😷，当然原因很多。</p><p>研究生毕业了我也终于要离开这20年呆的学校了。以后可能有机会再来，但真要离别的时候还是很伤感。各位后会有期，江湖再见。</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学生 </tag>
            
            <tag> 毕业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github Actions全自动博客部署</title>
      <link href="/Github_Actions_Blog_Automation_deploying.html"/>
      <url>/Github_Actions_Blog_Automation_deploying.html</url>
      
        <content type="html"><![CDATA[<p>博客的搭建可以参考<a href="https://darlewo.cn/blog_building.html">Hexo + Yilia + Github Pages 博客搭建</a>。</p><p>我们现在有两个仓库，一个是存放网页静态文件的<code>github.io</code>；一个是存放博客源码的仓库，因为里面的配置涉及到私人的密码信息，因此设置为私有仓库。</p><p>那么现在博客部署分为两步：</p><ol><li><p>当添加新文章或更改配置后，需要将源码push到私有仓库；</p></li><li><p>博客编译三步曲将网页静态文件上传到<code>github.io</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo cl</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></li></ol><p>有没有什么方法能实现全自动的博客部署， 每次只需要push源码到私有仓库，<code>hexo</code>会自动编译上传网页静态文件。</p><p>方法有很多，本文介绍一个<code>Github</code>新推出的功能–<code>Actions</code>.</p><a id="more"></a><p>首先介绍一下<code>Github Actions</code>:作为一种<code>CI/CD</code>工具(Continuous Integration/Continuous，持续集成/持续部署)它可以实现许多任务的自动化，能够进行测试，进行质量检查，然后部署。</p><p>这介绍有点官方，简而言之就是，当你将源代码push到Github之后，你可以自己定义一套操作流程。比如说你想让你的代码push上去之后在其他平台上看看会不会报错，那么你定义的流程就是首先将你上传的源码clone到本地（当然不是你的本地，类似于docker，都在云上），然后安装相关环境，再去执行你定义的操作。如果发现有什么错误信息，你好去修改你的源码。</p><p>拉回到本文的主题，我们想定义的一套流程是：当我push源码的时候，它会自动编译博客而不需要我再去执行那三步曲。</p><p>下面是具体步骤：</p><h2 id="准备密钥"><a class="header-anchor" href="#准备密钥"></a>准备密钥</h2><p>公钥+私钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">"Github邮箱地址"</span></span><br><span class="line"><span class="comment"># 比如 ssh-keygen -t rsa -C "123@gmail.com"</span></span><br></pre></td></tr></table></figure><p>公钥内容：<code>~/.ssh/id_rsa.pub</code></p><p>私钥内容：<code>~/.ssh/id_rsa</code></p><h2 id="密钥设置"><a class="header-anchor" href="#密钥设置"></a>密钥设置</h2><p>在github.io的Settings中,设置Deploy keys为公钥内容，命名随意。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200427092941218.png?x-oss-process=style/ossProcess" alt="image-20200427092941218"></p><p>在私有仓库的Settings中，设置Secrets，新增内容为私钥，命名为DEPLOY_KEY</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200427093208594.png?x-oss-process=style/ossProcess" alt="image-20200427093208594"></p><h2 id="添加Actions配置文件"><a class="header-anchor" href="#添加Actions配置文件"></a>添加Actions配置文件</h2><p>这一步就是定义当我们push源码后的操作</p><p>在Actions中新增<code>workflow</code></p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200427093833590.png?x-oss-process=style/ossProcess" alt="image-20200427093833590"></p><p><code>Github</code>会自动生成一个<code>yaml</code>文件</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200427094051530.png?x-oss-process=style/ossProcess" alt="image-20200427094051530"></p><p>替换为如下配置文件，只需要修改下<code>Git</code>的配置信息，点击<code>Start commit</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># workflow name</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">Hexo</span> <span class="string">Blog</span> <span class="string">CI</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># master branch on push, auto run</span></span><br><span class="line"><span class="attr">on:</span> </span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">master</span></span><br><span class="line">      </span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span> </span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span> </span><br><span class="line">        </span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="comment"># check it to your workflow can access it</span></span><br><span class="line">    <span class="comment"># from: https://github.com/actions/checkout</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">Repository</span> <span class="string">master</span> <span class="string">branch</span></span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment"># from: https://github.com/actions/setup-node  </span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Node.js</span> <span class="number">10.</span><span class="string">x</span> </span><br><span class="line">      <span class="attr">uses:</span> <span class="string">actions/setup-node@master</span></span><br><span class="line">      <span class="attr">with:</span></span><br><span class="line">        <span class="attr">node-version:</span> <span class="string">"10.x"</span></span><br><span class="line">    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Hexo</span> <span class="string">Dependencies</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">npm</span> <span class="string">install</span> <span class="string">hexo-cli</span> <span class="string">-g</span></span><br><span class="line">        <span class="string">npm</span> <span class="string">install</span> <span class="string">hexo-deployer-git</span> <span class="string">--save</span></span><br><span class="line">        <span class="string">npm</span> <span class="string">install</span></span><br><span class="line">    </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Deploy</span> <span class="string">Private</span> <span class="string">Key</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="attr">HEXO_DEPLOY_PRIVATE_KEY:</span> <span class="string">$&#123;&#123;</span> <span class="string">secrets.DEPLOY_KEY</span> <span class="string">&#125;&#125;</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">mkdir</span> <span class="string">-p</span> <span class="string">~/.ssh/</span></span><br><span class="line">        <span class="string">echo</span> <span class="string">"$HEXO_DEPLOY_PRIVATE_KEY"</span> <span class="string">&gt;</span> <span class="string">~/.ssh/id_rsa</span> </span><br><span class="line">        <span class="string">chmod</span> <span class="number">600</span> <span class="string">~/.ssh/id_rsa</span></span><br><span class="line">        <span class="string">ssh-keyscan</span> <span class="string">github.com</span> <span class="string">&gt;&gt;</span> <span class="string">~/.ssh/known_hosts</span></span><br><span class="line">        </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Git</span> <span class="string">Infomation</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span> </span><br><span class="line">        <span class="string">git</span> <span class="string">config</span> <span class="string">--global</span> <span class="string">user.name</span> <span class="string">"zengruizhao"</span></span><br><span class="line">        <span class="string">git</span> <span class="string">config</span> <span class="string">--global</span> <span class="string">user.email</span> <span class="string">"zzr_nuist@163.com"</span></span><br><span class="line">        <span class="string">git</span> <span class="string">config</span> <span class="string">--global</span> <span class="string">core.quotepath</span> <span class="literal">false</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span> <span class="string">Hexo</span> </span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">rm</span> <span class="string">-rf</span> <span class="string">.deploy_git</span></span><br><span class="line">        <span class="string">hexo</span> <span class="string">clean</span></span><br><span class="line">        <span class="string">hexo</span> <span class="string">generate</span> </span><br><span class="line">        <span class="string">hexo</span> <span class="string">deploy</span></span><br></pre></td></tr></table></figure><p>然后它会自动执行。当我们每次push源码的时候，<code>Github Actions</code>会自动执行上面的操作。比如我<code>git commit -m 'add 测试'</code>如下：</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200427094252744.png?x-oss-process=style/ossProcess" alt="image-20200427094252744"></p><h3 id="一切都是那么优雅–Elegant😄"><a class="header-anchor" href="#一切都是那么优雅–Elegant😄"></a>一切都是那么优雅–Elegant😄</h3><h2 id="参考"><a class="header-anchor" href="#参考"></a>参考</h2><ul><li><a href="https://mp.weixin.qq.com/s/6PaJyWcTB-YrQ50lUK7AFw" target="_blank" rel="noopener">如何用 GitHub Actions 写出高质量的 Python代码？</a></li><li><a href="https://blog.csdn.net/z_johnny/article/details/103910373" target="_blank" rel="noopener">Github Actions 通过 SSH 自动部署 Hexo</a></li><li><a href="https://www.bilibili.com/video/av69442934?p=1&amp;t=674" target="_blank" rel="noopener">【CICD】github新功能actions全方位讲解！！</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客开发--插入图片</title>
      <link href="/blog_inserting_images.html"/>
      <url>/blog_inserting_images.html</url>
      
        <content type="html"><![CDATA[<p>一身本领的孙悟空都需要一件趁手的兵器，我们博客开发当然也需要一套趁手的流程，让我们能够既快速又愉快地编辑博客。通过近段时间的摸索，我也从东海龙宫找到了一套在博客中插入图片的流程。</p><a id="more"></a><h1>Markdown编辑器–<a href="https://www.typora.io/" target="_blank" rel="noopener">Typora</a></h1><p>支持Windows，Linux和MacOS，相信我，用了之后你会离不开它。</p><h1>图床–<a href="https://www.aliyun.com/product/oss/" target="_blank" rel="noopener">阿里云OSS</a></h1><p>推荐使用阿里云搭建图床，用着舒服。</p><h1>图片上传工具–<a href="https://github.com/Molunerfinn/PicGo" target="_blank" rel="noopener">PicGo</a></h1><p>可以很方便的将图片上传到图床并获得各种格式的URL。同事PicGo支持Windows，Linux和MacOS。</p><h1>截图工具–<a href="https://getsharex.com/" target="_blank" rel="noopener">shareX</a>（非必须，其他截图工具如微信，qq都可以）</h1><p>可以选择截图后的操作，这也是个必备神器，以后慢慢研究。</p><h1>演示整个流程</h1><ol><li>阿里云OSS图床的搭建和PicGo的使用推荐<a href="http://jwxie.cn/posts/2020-03-27-215352_use_alibabaecs_for_pic_hosting/" target="_blank" rel="noopener">使用阿里云OSS搭建图床</a></li><li>如下是shareX的界面，设置截图后的动作如下两个选项。</li></ol><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422154810400.png?x-oss-process=style/ossProcess" alt="image-20200422154810400"></p><ol start="3"><li>Typora的设置如下，添加PicGo的安装路径即可。</li></ol><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422153704662.png?x-oss-process=style/ossProcess" alt="image-20200422153704662"></p><ol start="4"><li>当我们使用shareX截图之后可以直接将图片粘贴到Typora中，右键图片可以直接上传。</li></ol><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422153117322.png?x-oss-process=style/ossProcess" alt="image-20200422153117322"></p><p><strong>补充：这里也可以通过设置PicGo的快捷方式，当我们截图之后，通过快捷键直接上传并返回mardown格式的链接，直接复制到Typora中即可</strong>，这种方式更方便。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/20200430160305.png?x-oss-process=style/ossProcess" alt=""></p><ol start="5"><li>上传之后我们会发现图片的路径变成了URL，这样就可以直接上传博客了。</li></ol><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422153800002.png?x-oss-process=style/ossProcess" alt="image-20200422153800002"></p><hr><p><strong>流程的核心其实就是图床的搭建加上PicGo的使用，然后利用Typora编辑器去更好的编辑博客，其中ShareX只是一个比较好的截图工具，推荐大家使用</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nnU-Net</title>
      <link href="/nnU-Net.html"/>
      <url>/nnU-Net.html</url>
      
        <content type="html"><![CDATA[<h1>nnU-Net: Self-Adapting Framework for U-Net-Based Medical Image Segmentation</h1><p><a href="https://github.com/MIC-DKFZ/nnUNet" target="_blank" rel="noopener">Github</a>   <a href="https://arxiv.org/abs/1809.10486" target="_blank" rel="noopener">PDF</a></p><h2 id="Abstract"><a class="header-anchor" href="#Abstract"></a>Abstract</h2><p>​<a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">U-net</a>在2015年被提出来，到现在引用量已经1万大几了，真是厉害的很。目前在医学图像分割甚至在自然图像领域都应用广泛。但是U-net在针对不同任务的时候，它的网络结构，预处理，训练和推断可能都不同。这些选择不是相互独立的并且影响着最终的效果。作者提出了nnU-Net(no new-Net，起名字就是这么潇洒，学着点)，该网络是在2D和3D U-Net基础的一个鲁棒并且自适应的框架。作者认为不应该过多考虑网络设计的细节，而是应该关注于能提高模型性能和泛化性的其他方面。然后作者进行了实验发现效果惊人的好，我们就好好剖析一下，看看是怎么个惊人法。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/20200421152700.png?x-oss-process=style/ossProcess" alt=""></p><a id="more"></a><h2 id="Introduction"><a class="header-anchor" href="#Introduction"></a>Introduction</h2><p>​如果一个网络对指定的任务没有进行完全地手动优化，从而可以有大量的空间进行调整去改善结果，那么很容易证实这些用来提高网络性能的结构调整可以很有效（翻译的有点绕口）。在作者之前的实验中，这些调整并没有提高分割的性能并且不能推进SOTA（感同身受啊，我之前做胰腺的CT 3D分割，无论怎么加模块（记得当时用了各种很火的注意力机制，并且看很多新论文又推出了什么什么牛B的模块），调整结构，效果怎么也不能再往上提高了，一度怀疑我是否已经达到了SOTA 😂)。这让我们相信非结构因素在分割方法中更重要，同时它也被严重低估（记得我当时也用了很多预处理方法，还有在线的数据增强）。nnU-Net使用了三个相对简单的U-Net，并且这些U-Net只是对原始U-Net进行了很小的修改，并且没有使用各种扩展插件（残差连接、Dense连接还有各种注意力机制）。<strong>nnU-Net可以自动将其架构适应给定的图像几何，更重要的是，它彻底定义了围绕他们的所有其他步骤（听着很霸气）</strong>。这些步骤包括：</p><ol><li>预处理，比如resampling和normalization</li><li>训练，比如损失函数，优化器的设置和数据扩充</li><li>推断，比如基于图像块的策略，TTA（test-time augmentation）集成和模型集成</li><li>后处理，比如增强单连通域</li></ol><h2 id="Methods"><a class="header-anchor" href="#Methods"></a>Methods</h2><h3 id="网络结构"><a class="header-anchor" href="#网络结构"></a>网络结构</h3><p>​作者提出了3个网络，分别是2D U-Net、3D U-Net和级联Unet。2D和3D U-Net可以生成全分辨率的结果，级联网络的第一级产生一个低分辨率结果，第二级对它进行优化。</p><ul><li>改动：使用Leaky ReLU(neg.slope 1e-2)取代ReLU，使用Instance normalization取代Batchnormalization(<a href="https://blog.csdn.net/z13653662052/article/details/84503024" target="_blank" rel="noopener">解释</a>)</li><li><strong>2D U-Net</strong>: 既然使用了3D U-Net，为啥还有用2D的。因为有<a href="https://arxiv.org/pdf/1707.00587.pdf" target="_blank" rel="noopener">证据</a>（这证据也是作者发现的）表明当数据是各向异性的时候，传统的3D分割方法就会很差（😱怪不得我之前的效果一直上不去，难道是因为我用3D分割方法的原因？）</li><li><strong>3D U-Net</strong>: 3D网络固然好，就是太占用GPU显存。那我们可以使用小一点的图像块去训练，但这对于那些较大的图像例如肝脏，这种基于块的方法就会阻碍训练。这是因为受限于感受野的大小，网络结构不能收集足够的上下文信息去正确的识别肝脏和其他器官。</li><li><strong>级联3D U-Net</strong>：为了解决3D U-Net在大图像尺寸数据集上的缺陷，本文提出了级联模型。首先第一级3D U-Net在下采样的图像上进行训练，然后将结果上采样到原始的体素spacing。将上采样的结果作为一个额外的输入通道（one-hot编码）送入第二级3D U-Net，并使用基于图像块的策略在全分辨率的图像上进行训练。</li></ul><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200421180834404.png?x-oss-process=style/ossProcess" alt="image-20200421180834404"></p><h3 id="网络拓扑的动态自适应"><a class="header-anchor" href="#网络拓扑的动态自适应"></a>网络拓扑的动态自适应</h3><p>​由于输入图像大小的不同，输入图像块大小和每个轴池化操作的数量（同样也是卷积层的数量）必须能够自适应每个数据集去考虑充足的空间信息聚合。除了自适应图像几何，还需要考虑显存的使用。指导方针是动态平衡batch-size和网络容量。</p><p>​首先展示一下<strong>网络初始配置</strong>：</p><ul><li><strong>2D U-Net</strong>, 图像大小=256X256，batch size=42，最高层的特征图谱数量=30（每个下采样特征图谱数量翻倍），<strong>自动将这些参数调整为每个数据集的中值平面大小</strong>（这里使用面内间距最小的平面，对应于最高的分辨率），以便网络有效地训练整个切片。我们将网络配置为沿每个轴池化，直到该轴的特征图谱小于8（但最多不超过6个池化操作）。</li><li><strong>3D U-Net</strong>, 图像大小=128X128X128, batch size=2, 最高层的特征图谱数量=30。由于显存限制，不去增加图像大小超过128<sup>3</sup>体素，而是匹配输入图像和数据集中体素中值大小的比率。如果数据集的形状中值比128<sup>3</sup>小，那就使用形状的中值作为输入的图像大小并且增加batch size（目的是将体素的数量和128X128X128大小，batch size为2的体素数量相等）。沿每个轴最多池化5次直到特征图谱大小为8。</li></ul><p>在每个优化步骤中，我们限制每个网络的体素数量（输入图像的体素 X batch size）最多为数据集的5%。如果超过，则降低batch size(最小为2)。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422102321638.png?x-oss-process=style/ossProcess" alt="image-20200422102321638"></p><p>​可以看到不同数据集的输入图像块大小是不同的，3D U-Net lowres 为级联3D U-Net的第一级，第二级的配置和3D U-Net网络一致。</p><h3 id="预处理"><a class="header-anchor" href="#预处理"></a>预处理</h3><ul><li><p>Cropping：将所有数据的非零区域进行裁剪</p></li><li><p>Resampling：我们知道医学影像数据不同的设备和设置会导致具有不同体素间距的数据，而CNN并不能理解这种体素间距的概念（别说CNN了，我一开始也不懂）。为了让网络学会空间语义，所有的病例重采样到相应数据集的体素间距中值，对数据和mask分别使用三阶样条差值和最近邻差值方法。</p><p>注：如果重采样数据的形状中值可以作为3D U-Net中的输入图像（batch size=2）的4倍以上，则<strong>使用级联U-Net网络</strong>，且数据集需要重新采样到较低的分辨率。可以以2为因子增加体素间距（降低精度）直到4倍那个条件不满足。如果该数据集是各向异性的（比如1.2mm X 1.2mm X 3.5mm），首先更高分辨率的轴进行下采样直到与低分辨率轴匹配，然后才对所有轴同时进行下采样。</p></li><li><p>Normalization：对于CT图像，首先搜集分割mask内的像素值，然后所有的数据截断到这些像素值的[0.5, 99.5]%，然后进行z-score标准化；对于MRI图像，直接进行z-score标准化。</p><p>注：如果因为裁剪减少了病例平均大小的1/4或更多，则标准化只在非零元素的mask内部进行，并且mask外的所有值设为0。</p></li></ul><h3 id="训练"><a class="header-anchor" href="#训练"></a>训练</h3><ul><li><p>5折交叉验证</p></li><li><p>Loss = Dice + Cross Entropy</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422090036238.png?x-oss-process=style/ossProcess" alt="image-20200422090036238"></p><p>u为模型概率输出，v是ground Truth的one hot编码，k为类别数。</p></li><li><p>优化器：Adam，初始学习率3e-4，每个epoch有250个batch。</p></li><li><p>学习率调整策略：计算训练集和验证集的指数滑动平均loss，如果训练集的指数滑动平均loss在近30个epoch内减少不够5e-3，则学习率衰减5倍。</p></li><li><p>训练停止条件：当学习率大于10<sup>-6</sup>且验证集的指数滑动平均loss在近60个epoch内减少不到5e-3，则终止训练。</p></li><li><p>数据扩充（On the fly）：随机旋转，缩放，elastic deformation，gamma矫正和镜像。<a href="https://github.com/MIC-DKFZ/batchgenerators" target="_blank" rel="noopener">代码</a></p><p>注：如果3D U-Net的输入图像块尺寸的最大边长是最短边长的两倍以上，这种情况对每个2维面做数据增广。</p></li><li><p>级联U-Net的第二级接受前一级的输出作为输入的一部分，为了防止强co-adaptation，应用随机形态学操作（腐蚀、膨胀、开运算、闭运算）去随机移除掉这些分割结果的连通域。</p></li><li><p>图像块采样：为了增强网络训练的稳定性，强制每个batch中超过1/3的样本包含至少一个随机选择的前景。</p></li></ul><h3 id="推断"><a class="header-anchor" href="#推断"></a>推断</h3><p>​所有的推断都是基于图像块（patch-wise）。网络精度沿着图像块边缘逐渐降低，所以提高图像块中心的体素权重，降低边缘权重。图像块重叠大小为size / 2，并使用TTA（沿各个轴镜像）进行集成。在测试集上测试时使用5个交叉训练的模型进行集成来提高模型的鲁棒性。</p><h3 id="后处理"><a class="header-anchor" href="#后处理"></a>后处理</h3><p>​一般认为某一类是在单连通域内，也就是说一个病例内只会有一个这样的区域。那么只保留最大的连通域，将其他的所有小连通域去掉。</p><h2 id="实验"><a class="header-anchor" href="#实验"></a>实验</h2><p><a href="http://medicaldecathlon.com/index.html" target="_blank" rel="noopener">竞赛官网</a></p><p>​这个竞赛的目的就是开发一套方法能适应不同的医学影像数据，下面是作者在训练集上交叉验证的结果和在测试集上的结果。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422102230285.png?x-oss-process=style/ossProcess" alt="image-20200422102230285"></p><p>​可以看到除了在脑肿瘤分割任务中测试集上性能下降有点大，在其他数据集上简直就是完美（相对于在训练集的交叉验证结果）。作者也解释了这是因为脑肿瘤的数据和Ground Truth之间存在差异（怪数据，不怪模型）。在<a href="https://decathlon.grand-challenge.org/evaluation/results" target="_blank" rel="noopener">竞赛官网</a>可以看到本文排名第一（🐮哄哄）。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422103457661.png?x-oss-process=style/ossProcess" alt="image-20200422103457661"></p><h2 id="总结"><a class="header-anchor" href="#总结"></a>总结</h2><p>这里借用<a href="https://zhuanlan.zhihu.com/p/100014604" target="_blank" rel="noopener">张良怀</a>的总结和分析</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/image-20200422103803031.png?x-oss-process=style/ossProcess" alt="image-20200422103803031"></p><p>​作者也说了，文章中的各种改进也并没有理论实验支撑，比如Leaky ReLU替换ReLU，数据扩充方法。因此需要做更多的消融实验去证明。</p><p>作者有更详细的版本<a href="https://arxiv.org/abs/1904.08128" target="_blank" rel="noopener">Automated Design of Deep Learning Methods for Biomedical Image Segmentation</a>，推荐精读。</p><hr><h1>我的感悟</h1><p>​通读这篇文章的原因是因为他的成绩很好，想一探究竟。回想我们以前去做这样的任务的时候，往往会在模型上花更多的时间。比如添加各种模块，像常用的残差连接，Dense连接，还有各种各样的注意力机制，或者增加减少特征图谱的数量。有时候这种模型的改进似乎真的提高了效果，但归根结底这种效果也只是拼凑出来的，具体为什么能这样，论文作者也说不出个所以然来，而且模型的泛化性值得考量。本文作者就是用最原始的U-Net通过模型调整以外的一些操作来达到更好的效果。当然这有点打比赛的意味，为了最终结果不考虑测试时间，离最终的临床似乎有点偏离。但我觉得这是一种进步，一种跳出思维定式的进步。他打碎了我们对网络结构调整的这种痴念，让我们回归到正常的科研道路上。当然不是说网络结构调整没有用，只是我觉得现阶段网络结构调整应该是一种进化式的，比如残差网络。而说不好听一点目前很多网络的改进只是为了灌水（当然作者本意可能不是如此），没有太多实际价值。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫入门，一步步走上人(F)生(Z)巅(道)峰(路)--高级1</title>
      <link href="/getting_started_with_crawler_advanced_4.html"/>
      <url>/getting_started_with_crawler_advanced_4.html</url>
      
        <content type="html"><![CDATA[<h1><a href="https://www.osgeo.cn/scrapy/" target="_blank" rel="noopener">Scrapy</a></h1><h2 id="架构图"><a class="header-anchor" href="#架构图"></a>架构图</h2><p><img src="https://upload-images.jianshu.io/upload_images/12983160-796fd5bfb38a39ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt=""></p><a id="more"></a><ul><li>Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</li><li>Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。</li><li>Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，</li><li>Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，</li><li>Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.</li><li>Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。</li><li>Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12983160-394d93c87390c455.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1110/format/webp" alt=""></p><h2 id="创建项目和爬虫："><a class="header-anchor" href="#创建项目和爬虫："></a>创建项目和爬虫：</h2><ol><li>创建项目：`scrapy startproject 项目名字</li><li>创建爬虫：进入到项目所在的路径，执行命令：<code>scrapy genspider 爬虫名字 爬虫域名</code>。注意：爬虫名字和项目名字不能一致。</li></ol><h2 id="项目目录结构"><a class="header-anchor" href="#项目目录结构"></a>项目目录结构</h2><ol><li><code>items.py</code>:用来存放爬虫爬取下来数据的模型</li><li><code>middlewares.py</code>：用来存放各种中间件的文件。</li><li><code>pipelines.py</code>:用来将<code>items</code>的模型存储到本地磁盘中。</li><li><code>settings.py</code>：爬虫的一些配置信息（如请求头，多久发送一次请求，ip代理池等）</li><li><code>scrapy.cfg</code>：项目的配置文件。</li><li><code>spiders</code>包：以后所有的爬虫，都是存放到这里。</li></ol><h2 id="快速入门"><a class="header-anchor" href="#快速入门"></a>快速入门</h2><ol><li><p>response是一个<code>scrapy.http.response.html.HtmlResonse</code>对象，可以执行<code>xpath</code>和<code>css</code>语法来提取数据。</p></li><li><p>提取出来的数据是一个<code>Selector</code>或<code>SelectorList</code>对象。获取其中字符串应该执行<code>getall</code>或<code>get</code>方法。</p></li><li><p><code>getall</code>方法：获取<code>Selector</code>中的所有文本，返回一个列表。</p></li><li><p><code>get</code>方法：获取的是<code>Selector</code>中的第一个文本，返回一个<code>str</code>类型。</p></li><li><p>如果数据解析回来，要传给<code>pipeline</code>处理，可以使用<code>yield</code>来返回。或者收集所有的<code>item</code>,最后统一使用return返回。</p></li><li><p><code>item</code>：建议在<code>items.py</code>中定义好模型，以后就不用使用字典的方式。</p></li><li><p>`pipeline:这个是专门用来保存数据的，其中有三个方法会经常使用：</p><ul><li><code>open_spider(self, spider)</code>：当爬虫被打开的时候执行</li><li><code>process_item(self, item, spider)</code>:当爬虫有<code>item</code>传过来的时候会被调用。</li><li><code>closs_spider(self, spider)</code>: 当爬虫关闭的时候回被调用</li></ul><p>要激活<code>pipeline</code>，应该在<code>settings.py</code>中，设置<code>ITEM_PIPELINES</code>。</p></li><li><p>保存<code>json</code>数据的时候可以使用<code>JsonItemExporter</code>和<code>JsonLinesItemExporter</code></p><ul><li><p><code>JsonItemExporter</code>:每次把数据添加到内存中，最后统一写入到磁盘中。好处是存储的数据是一个满足<code>json</code>规则的数据。坏处是如果数据量比较大，那么比较耗内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = open(<span class="string">'duanzi.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line">        self.exporter = JsonItemExporter(self.fp, ensure_ascii=<span class="literal">False</span>,</span><br><span class="line">                                         encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure></li><li><p><code>JsonLinesItemExporter</code>：每次调用<code>exorter_item</code>的时候就把<code>item</code>存储到磁盘中。坏处是每一个字典是一行，整个文件不是一个满足<code>json</code>格式的文件。好处是每次处理数据的时候就直接存储到磁盘中，不会耗内存，数据也比较安全。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonLinesItemExporter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = open(<span class="string">'duanzi.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line">        self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=<span class="literal">False</span>,</span><br><span class="line">                                         encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure></li></ul></li></ol><h2 id="CrawlSpider"><a class="header-anchor" href="#CrawlSpider"></a>CrawlSpider</h2><p><strong>在创建爬虫的命令修改为<code>scrapy genspider -t crawl 爬虫名字 爬虫域名</code></strong></p><p>需要使用<code>LinkExtractor</code>和<code>Rule</code>，这两个东西决定爬虫的具体走向。</p><ol><li><code>allow</code>设置规则的方法，要能够限制在我们想要的<code>url</code>上，不要跟其他的<code>url</code>产生相同的正则表达式即可。</li><li>什么情况下使用<code>follow</code>:如果在爬取页面的 时候，需要将满足当前条件的<code>url</code>再进行跟进，那么就设置为True,否则设置为False.</li><li>什么情况下指定<code>callback</code>：如果这个<code>url</code>对应的页面，只是为了获取更多的<code>url</code>， 并不需要里面的数据，那么可以不指定<code>callback</code>；如果想要获取<code>url</code>对应页面的数据，那么就需要指定一个<code>callback</code>。</li></ol><h2 id="Scrapy-Shell"><a class="header-anchor" href="#Scrapy-Shell"></a>Scrapy Shell</h2><ol><li>可以方便我们做一些数据提取的测试代码</li><li>如果想要执行<code>scrapy</code>命令，毫无疑问是要先进入到<code>scrapy</code>所在的环境中</li><li>如果先要读取某个项目的配置信息，那么应该先进入到这个项目中，再执行<code>scrapy shell</code>命令</li></ol><h2 id="模拟登陆人人网"><a class="header-anchor" href="#模拟登陆人人网"></a>模拟登陆人人网</h2><ol><li>想要发送<code>post</code>请求，推荐使用<code>scrapy.FormRequest</code>方法，可以方便的指定表单数据。</li><li>如果想在爬虫一开始的时候就发送<code>post</code>请求，那么应该重写<code>start_requests</code>方法，在这个方法中发送<code>post请求</code></li></ol><h1>参考</h1><ul><li><a href="https://www.bilibili.com/video/av57909837?p=1" target="_blank" rel="noopener">python爬虫_从入门到精通（高级篇）scrapy框架、反爬、分布式爬虫</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫入门，一步步走上人(F)生(Z)巅(道)峰(路)--进阶3</title>
      <link href="/getting_started_with_crawler_advanced_3.html"/>
      <url>/getting_started_with_crawler_advanced_3.html</url>
      
        <content type="html"><![CDATA[<h1>正则表达式和<code>re</code>模块</h1><p>世界上分两种人，一种是懂正则表达式的，一种是不懂正则表达式的。</p><a id="more"></a><h2 id="匹配某个字符串"><a class="header-anchor" href="#匹配某个字符串"></a>匹配某个字符串</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">'hello'</span></span><br><span class="line">ret = re.match(<span class="string">'he'</span>, text)</span><br><span class="line">print(ret.group())</span><br><span class="line">&gt;&gt; he</span><br></pre></td></tr></table></figure><h2 id="匹配单个字符"><a class="header-anchor" href="#匹配单个字符"></a>匹配单个字符</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ret = re.match(<span class="string">'.'</span>, text) <span class="comment"># 匹配任意字符</span></span><br><span class="line">ret = re.match(<span class="string">'\s'</span>, text) <span class="comment"># 匹配空白字符（\n, \t, \r, 空格）</span></span><br><span class="line">ret = re.match(<span class="string">'[a1]'</span>, text) <span class="comment"># []组合的方式，只要满足中括号中的某一项都算匹配成功</span></span><br><span class="line">ret = re.match(<span class="string">'\d'</span>, text) <span class="comment"># 匹配任意数字（0-9）</span></span><br><span class="line">ret = re.match(<span class="string">'[0-9]'</span>, text)</span><br><span class="line">ret = re.match(<span class="string">'\D'</span>, text) <span class="comment"># 匹配任意非数字</span></span><br><span class="line">ret = re.match(<span class="string">'[^0-9]'</span>, text)</span><br><span class="line">ret = re.match(<span class="string">'\w'</span>, text) <span class="comment"># 匹配a-z，A-Z，数字和下划线</span></span><br><span class="line">ret = re.match(<span class="string">'[0-9a-zA-Z_]'</span>, text)</span><br><span class="line">ret = re.match(<span class="string">'\W'</span>, text) <span class="comment"># 与\w正好相反</span></span><br><span class="line">ret = re.match(<span class="string">'[^0-9a-zA-Z_]'</span>, text)</span><br></pre></td></tr></table></figure><h2 id="匹配多个字符"><a class="header-anchor" href="#匹配多个字符"></a>匹配多个字符</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ret = re.match(<span class="string">'a*'</span>, text) <span class="comment"># 匹配0或者任意多个字符</span></span><br><span class="line">ret = re.match(<span class="string">'a+'</span>, text) <span class="comment"># 匹配1或者任意多个字符</span></span><br><span class="line">ret = re.match(<span class="string">'a?'</span>, text) <span class="comment"># 匹配0或者1个字符</span></span><br><span class="line">ret = re.match(<span class="string">'a&#123;m&#125;'</span>, text) <span class="comment"># 匹配m个字符</span></span><br><span class="line">ret = re.match(<span class="string">'a&#123;m, n&#125;'</span>, text) <span class="comment"># 匹配m到n个字符</span></span><br></pre></td></tr></table></figure><h2 id="小案例"><a class="header-anchor" href="#小案例"></a>小案例</h2><ol><li><p>验证手机号码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'18565659587'</span></span><br><span class="line">ret = re.match(<span class="string">'1[34578]\d&#123;9&#125;'</span>, text)</span><br></pre></td></tr></table></figure></li><li><p>验证邮箱</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'78adf_@qq.com'</span></span><br><span class="line">ret = re.match(<span class="string">'\w+@[0-9a-z]+\.[a-z]+'</span>, text)</span><br></pre></td></tr></table></figure></li><li><p>验证<code>url</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'https://www.bilibili.com/video/BV1Lx41197NM?p=42'</span></span><br><span class="line">ret = re.match(<span class="string">'(http|https|ftp)://[^\s]+'</span>, text)</span><br></pre></td></tr></table></figure></li><li><p>验证身份证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'14112119851101002X'</span></span><br><span class="line">ret = re.match(<span class="string">'\d&#123;17&#125;[0-9xX]'</span>, text)</span><br></pre></td></tr></table></figure></li></ol><h2 id="补充"><a class="header-anchor" href="#补充"></a>补充</h2><h3 id="脱字号-：表示以…为开始"><a class="header-anchor" href="#脱字号-：表示以…为开始"></a><code>^</code>(脱字号)：表示以…为开始</h3><p>如果在中括号中代表取反</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'hello'</span></span><br><span class="line">ret = re.match(<span class="string">'^h'</span>, text) <span class="comment"># 以h为开头</span></span><br></pre></td></tr></table></figure><h3 id="表示以…结束"><a class="header-anchor" href="#表示以…结束"></a><code>$</code>:表示以…结束</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'jfaksd@163.com'</span></span><br><span class="line">ret = re.match(<span class="string">'\w+@163.com$'</span>, text) <span class="comment"># 以@163.com结尾</span></span><br></pre></td></tr></table></figure><h3 id="匹配多个表达式或者字符串"><a class="header-anchor" href="#匹配多个表达式或者字符串"></a><code>|</code>:匹配多个表达式或者字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">''</span></span><br><span class="line">ret = re.match(<span class="string">'ftp|https|http'</span>, txt)</span><br></pre></td></tr></table></figure><h3 id="贪婪模式和非贪婪模式"><a class="header-anchor" href="#贪婪模式和非贪婪模式"></a>贪婪模式和非贪婪模式</h3><ul><li>贪婪模式(default)：正则表达式会匹配尽量多的字符</li><li>非贪婪模式：正则表达式会尽量少的匹配字符</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'&lt;h1&gt;标题&lt;/h1&gt;'</span></span><br><span class="line"><span class="comment"># 贪婪模式</span></span><br><span class="line">ret = re.match(<span class="string">'&lt;h.+&gt;'</span>, text)</span><br><span class="line">print(ret.group())</span><br><span class="line">&gt;&gt; &lt;h1&gt;标题&lt;/h1&gt;</span><br><span class="line"><span class="comment"># 非贪婪模式</span></span><br><span class="line">ret = re.match(<span class="string">'&lt;h.+?&gt;'</span>, text)</span><br><span class="line">print(ret.group())</span><br><span class="line">&gt;&gt; &lt;h1&gt;</span><br></pre></td></tr></table></figure><h3 id="案例：匹配0-100之间的任意数字"><a class="header-anchor" href="#案例：匹配0-100之间的任意数字"></a>案例：匹配0-100之间的任意数字</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'100'</span></span><br><span class="line">ret = re.match(<span class="string">'[1-9]\d?$|100$|0$'</span>, text)</span><br><span class="line">print(ret.group())</span><br></pre></td></tr></table></figure><h2 id="re模块常用函数"><a class="header-anchor" href="#re模块常用函数"></a><code>re</code>模块常用函数</h2><h3 id="match"><a class="header-anchor" href="#match"></a><code>match</code></h3><p>从开始的位置进行匹配，如果开始位置没有匹配到，就失败。</p><h3 id="search"><a class="header-anchor" href="#search"></a><code>search</code></h3><p>在字符串中找满足条件的字符，如果找到就返回。即只会找到第一个满足条件的字符。</p><h3 id="分组"><a class="header-anchor" href="#分组"></a>分组</h3><p>在正则表达式中，可以对过滤到的字符串进行分组，分组使用圆括号的方式。</p><ol><li><p><code>group</code>:和<code>group(0)</code>是等价的，返回的是对整个满足条件的字符串；</p></li><li><p><code>groups</code>:返回的是里面的子组，索引从1开始；</p></li><li><p><code>group(1)</code>:返回第一个子组，可以传入多个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'apple price is $99, orange price is $18'</span></span><br><span class="line">ret = re.search(<span class="string">r'.*(\$[0-9]+).*(\$[0-9]+)'</span>, text)</span><br><span class="line">print(ret.group())</span><br><span class="line">print(ret.group(<span class="number">0</span>))</span><br><span class="line">print(ret.group(<span class="number">1</span>))</span><br><span class="line">print(ret.groups())</span><br><span class="line">&gt;&gt; apple price <span class="keyword">is</span> $<span class="number">99</span>, orange price <span class="keyword">is</span> $<span class="number">18</span></span><br><span class="line">&gt;&gt; apple price <span class="keyword">is</span> $<span class="number">99</span>, orange price <span class="keyword">is</span> $<span class="number">18</span></span><br><span class="line">&gt;&gt; $<span class="number">99</span></span><br><span class="line">&gt;&gt; (<span class="string">'$99'</span>, <span class="string">'$18'</span>)</span><br></pre></td></tr></table></figure></li></ol><h3 id="findall"><a class="header-anchor" href="#findall"></a><code>findall</code></h3><p>找出所有满足条件的，返回一个列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'apple price is $99, orange price is $18'</span></span><br><span class="line">ret = re.findall(<span class="string">r'[0-9]+'</span>, text)</span><br><span class="line">print(ret)</span><br><span class="line">&gt;&gt; [<span class="string">'99'</span>, <span class="string">'18'</span>]</span><br></pre></td></tr></table></figure><h3 id="sub"><a class="header-anchor" href="#sub"></a><code>sub</code></h3><p>替换字符串，将匹配到的字符串替换为其他字符串。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'apple price is $99, orange price is $18'</span></span><br><span class="line">ret = re.sub(<span class="string">r'\$[0-9]+'</span>,<span class="string">'0'</span>, text)</span><br><span class="line">print(ret)</span><br><span class="line">&gt;&gt; apple price <span class="keyword">is</span> <span class="number">0</span>, orange price <span class="keyword">is</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="split"><a class="header-anchor" href="#split"></a><code>split</code></h3><p>使用正则表达式分割字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'hello world ni hao'</span></span><br><span class="line">ret = re.split(<span class="string">r'\W'</span>, text)</span><br><span class="line">print(ret)</span><br><span class="line">&gt;&gt; [<span class="string">'hello'</span>, <span class="string">'world'</span>, <span class="string">'ni'</span>, <span class="string">'hao'</span>]</span><br></pre></td></tr></table></figure><h3 id="compile"><a class="header-anchor" href="#compile"></a><code>compile</code></h3><p>对于一些经常用到的正则表达式，可以使用<code>compile</code>进行编译，后期使用的时候就可以直接拿过来使用，执行效率会更快。而且可以添加注释，方便查看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'the number is 20.50'</span></span><br><span class="line">r = re.compile(<span class="string">r"""</span></span><br><span class="line"><span class="string">    \d+ # 小数点前面的数字</span></span><br><span class="line"><span class="string">    \.? # 小数点</span></span><br><span class="line"><span class="string">    \d* # 小数点后面的数字</span></span><br><span class="line"><span class="string">    """</span>, re.VERBOSE)</span><br><span class="line">ret = re.search(r, text)</span><br><span class="line">print(ret.group())</span><br><span class="line">&gt;&gt; <span class="number">20.50</span></span><br></pre></td></tr></table></figure><h1>参考</h1><ul><li><a href="https://www.bilibili.com/video/BV1Lx41197NM?p=40" target="_blank" rel="noopener">python爬虫_从入门到精通（基础篇）http协议、requests库、beautifulsoup库、正则表达式、存取、lxml、xpath、css选择器</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫入门，一步步走上人(F)生(Z)巅(道)峰(路)--进阶-实战</title>
      <link href="/getting_started_with_crawler_advanced_practice.html"/>
      <url>/getting_started_with_crawler_advanced_practice.html</url>
      
        <content type="html"><![CDATA[<p><strong>使用三种方式对网页信息进行提取</strong></p><ul><li><strong>XPath语法和lxml模块</strong></li><li><strong>BeautifulSoup4</strong></li><li><strong>正则表达式</strong></li></ul><a id="more"></a><h1>豆瓣电影</h1><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_oQaLDYrAba.png?x-oss-process=style/ossProcess" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://movie.douban.com/cinema/nowplaying/nanjing/'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                  <span class="string">'Chrome/83.0.4090.0 Safari/537.36 Edg/83.0.467.0 '</span>,</span><br><span class="line">    <span class="string">'Referer'</span>: <span class="string">'https://movie.douban.com/'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">html = response.text</span><br><span class="line"></span><br><span class="line">htmlElement = etree.HTML(html)</span><br><span class="line">ul = htmlElement.xpath(<span class="string">"//ul[@class='lists']"</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">movies = []</span><br><span class="line">lis = ul.xpath(<span class="string">'./li'</span>)</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> lis:</span><br><span class="line">    title = li.xpath(<span class="string">'@data-title'</span>)[<span class="number">0</span>]</span><br><span class="line">    director = li.xpath(<span class="string">'@data-director'</span>)[<span class="number">0</span>]</span><br><span class="line">    actors = li.xpath(<span class="string">'@data-actors'</span>)[<span class="number">0</span>]</span><br><span class="line">    thumbnail = li.xpath(<span class="string">'.//img/@src'</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    movie = &#123;</span><br><span class="line">        <span class="string">'title'</span>: title,</span><br><span class="line">        <span class="string">'director'</span>: director,</span><br><span class="line">        <span class="string">'actors'</span>: actors,</span><br><span class="line">        <span class="string">'thumbnail'</span>: thumbnail</span><br><span class="line">    &#125;</span><br><span class="line">    movies.append(movie)</span><br><span class="line"></span><br><span class="line">print(movies)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output:[&#123;'title': '奇妙王国之魔法奇缘', 'director': '陈设', 'actors': '卢瑶 / 张洋 / 陈新玥', 'thumbnail': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2577837112.jpg'&#125;, &#123;'title': '大红包', 'director': '李克龙', 'actors': '包贝尔 / 李成敏 / 贾冰', 'thumbnail': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2581346773.jpg'&#125;, &#123;'title': '金禅降魔', 'director': '彭发 王凯 程中豪', 'actors': '释小龙 / 胡军 / 姚星彤', 'thumbnail': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2564190636.jpg'&#125;, &#123;'title': '82号古宅', 'director': '袁杰', 'actors': '葛天 / 扈天翼 / 黄心娣', 'thumbnail': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2586838530.jpg'&#125;, &#123;'title': '亲亲哒', 'director': '马雍', 'actors': '马良博一 / 卢小路 / 尹恒', 'thumbnail': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2579189777.jpg'&#125;, &#123;'title': '六月的秘密', 'director': '王暘', 'actors': '郭富城 / 苗苗 / 吴建飞', 'thumbnail': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2522497098.jpg'&#125;, &#123;'title': '秘密访客', 'director': '陈正道', 'actors': '郭富城 / 段奕宏 / 张子枫', 'thumbnail': 'https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2579398648.jpg'&#125;, &#123;'title': '无名狂', 'director': '李云波', 'actors': '张晓晨 / 隋咏良 / 上白', 'thumbnail': 'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2574800433.jpg'&#125;, &#123;'title': '刺杀小说家', 'director': '路阳', 'actors': '雷佳音 / 杨幂 / 董子健', 'thumbnail': 'https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2580314674.jpg'&#125;]</span><br></pre></td></tr></table></figure><h1>拉钩网职位信息</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://www.lagou.com/zhaopin/PHP/?labelWords=label'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                  <span class="string">'Chrome/83.0.4090.0 Safari/537.36 Edg/83.0.467.0 '</span>,</span><br><span class="line">    <span class="string">'Referer'</span>: <span class="string">'https://www.lagou.com/'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">html = response.text</span><br><span class="line"></span><br><span class="line">htmlElement = etree.HTML(html)</span><br><span class="line">ul = htmlElement.xpath(<span class="string">"//ul[@class='item_con_list']"</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">jobs = []</span><br><span class="line">lis = ul.xpath(<span class="string">'./li'</span>)</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> lis:</span><br><span class="line">    position = li.xpath(<span class="string">'@data-positionname'</span>)[<span class="number">0</span>]</span><br><span class="line">    company = li.xpath(<span class="string">'@data-company'</span>)[<span class="number">0</span>]</span><br><span class="line">    salary = li.xpath(<span class="string">'@data-salary'</span>)[<span class="number">0</span>]</span><br><span class="line">    website = li.xpath(<span class="string">".//a/@href"</span>)[<span class="number">0</span>]</span><br><span class="line">    education = li.xpath(<span class="string">".//div[@class='li_b_l']//text()"</span>)[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    job = &#123;</span><br><span class="line">        <span class="string">'position'</span>: position,</span><br><span class="line">        <span class="string">'company'</span>: company,</span><br><span class="line">        <span class="string">'salary'</span>: salary,</span><br><span class="line">        <span class="string">'website'</span>: website,</span><br><span class="line">        <span class="string">'education'</span>: education</span><br><span class="line">    &#125;</span><br><span class="line">    jobs.append(job)</span><br><span class="line"></span><br><span class="line">print(jobs)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output:[&#123;'position': '高级PHP开发工程师', 'company': '广州微用科技', 'salary': '10k-18k', 'website': 'https://www.lagou.com/jobs/6813986.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 大专\n                                '&#125;, &#123;'position': '高级PHP开发工程师', 'company': '互爱（北京）科技股份有限公司', 'salary': '25k-40k', 'website': 'https://www.lagou.com/jobs/6585859.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 不限\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '金山云', 'salary': '20k-40k', 'website': 'https://www.lagou.com/jobs/6871122.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验5-10年 / 不限\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '央视频融媒体', 'salary': '12k-20k', 'website': 'https://www.lagou.com/jobs/6873952.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验5-10年 / 本科\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '顺丰同城科技', 'salary': '20k-40k', 'website': 'https://www.lagou.com/jobs/5788250.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验1-3年 / 本科\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '红布林', 'salary': '15k-30k', 'website': 'https://www.lagou.com/jobs/6963665.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 本科\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '吉城美家', 'salary': '7k-14k', 'website': 'https://www.lagou.com/jobs/5553859.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 本科\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '有咖互动', 'salary': '10k-15k', 'website': 'https://www.lagou.com/jobs/6959578.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验1-3年 / 本科\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '微拍堂', 'salary': '20k-30k', 'website': 'https://www.lagou.com/jobs/6972670.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 本科\n                                '&#125;, &#123;'position': 'PHP开发工程师', 'company': '广州市搜游网络科技', 'salary': '20k-40k', 'website': 'https://www.lagou.com/jobs/6959547.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 大专\n                                '&#125;, &#123;'position': 'PHP高级开发工程师', 'company': '广州市搜游网络科技', 'salary': '25k-45k', 'website': 'https://www.lagou.com/jobs/6943052.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 大专\n                                '&#125;, &#123;'position': 'PHP高级开发工程师', 'company': '明源云', 'salary': '15k-25k', 'website': 'https://www.lagou.com/jobs/4619045.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 本科\n                                '&#125;, &#123;'position': 'PHP开发工程师', 'company': '微通', 'salary': '8k-16k', 'website': 'https://www.lagou.com/jobs/501604.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验不限 / 不限\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '掌阅', 'salary': '15k-25k', 'website': 'https://www.lagou.com/jobs/6237409.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验3-5年 / 本科\n                                '&#125;, &#123;'position': 'php开发工程师', 'company': '经传多赢', 'salary': '10k-18k', 'website': 'https://www.lagou.com/jobs/6820316.html?show=0d9c9879671444a5926680abc20322be', 'education': '经验1-3年 / 本科\n                                '&#125;]</span><br></pre></td></tr></table></figure><h1>中国天气网</h1><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_O4dy4reTS3.png?x-oss-process=style/ossProcess" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Bar</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">from</span> pyecharts.globals <span class="keyword">import</span> ThemeType</span><br><span class="line"></span><br><span class="line">Temp = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePages</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                      <span class="string">'Chrome/83.0.4090.0 Safari/537.36 Edg/83.0.467.0 '</span></span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    text =response.content.decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment"># soup = BeautifulSoup(text, 'lxml')</span></span><br><span class="line">    soup = BeautifulSoup(text, <span class="string">'html5lib'</span>)</span><br><span class="line">    conMidtab = soup.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'conMidtab'</span>&#125;)</span><br><span class="line">    tables = conMidtab.find_all(<span class="string">'table'</span>)</span><br><span class="line">    <span class="keyword">for</span> table <span class="keyword">in</span> tables:</span><br><span class="line">        trs = table.find_all(<span class="string">'tr'</span>)[<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">for</span> index, tr <span class="keyword">in</span> enumerate(trs):</span><br><span class="line">            infors = list(tr.stripped_strings)</span><br><span class="line">            city = infors[<span class="number">1</span>] <span class="keyword">if</span> index == <span class="number">0</span> <span class="keyword">else</span> infors[<span class="number">0</span>]</span><br><span class="line">            minTemp = infors[<span class="number">-2</span>]</span><br><span class="line">            Temp.append(&#123;<span class="string">'city'</span>: city, <span class="string">'minTemp'</span>: int(minTemp)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    urls = [<span class="string">'hb'</span>, <span class="string">'db'</span>, <span class="string">'hd'</span>, <span class="string">'hz'</span>, <span class="string">'hn'</span>, <span class="string">'xn'</span>, <span class="string">'xb'</span>, <span class="string">'gat'</span>]</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        url = <span class="string">f'http://www.weather.com.cn/textFC/<span class="subst">&#123;url&#125;</span>.shtml'</span></span><br><span class="line">        parsePages(url)</span><br><span class="line"></span><br><span class="line">    Temp.sort(key=<span class="keyword">lambda</span> data:data[<span class="string">'minTemp'</span>])</span><br><span class="line">    data = Temp[:<span class="number">10</span>]</span><br><span class="line">    cities = list(map(<span class="keyword">lambda</span> x: x[<span class="string">'city'</span>], data))</span><br><span class="line">    minTemp = list(map(<span class="keyword">lambda</span> x:x[<span class="string">'minTemp'</span>], data))</span><br><span class="line">    chart = Bar(init_opts=opts.InitOpts(theme=ThemeType.LIGHT, page_title=<span class="string">'中国最低气温排行榜'</span>))</span><br><span class="line">    chart.add_xaxis(cities)</span><br><span class="line">    chart.add_yaxis(<span class="string">'最低气温'</span>, minTemp)</span><br><span class="line">    chart.render()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/chrome_gq5P1Qy3G3.png?x-oss-process=style/ossProcess" alt=""></p><h1>古诗文网</h1><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_XeyhU1btEl.png?x-oss-process=style/ossProcess" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePages</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                      <span class="string">'Chrome/83.0.4090.0 Safari/537.36 Edg/83.0.467.0 '</span></span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    text = response.text</span><br><span class="line">    rTitles = re.compile(<span class="string">r'&lt;div class="cont"&gt;.*?&lt;b&gt;(.*?)&lt;/b&gt;'</span>, re.S)</span><br><span class="line">    rDynasties = re.compile(<span class="string">r'&lt;div class="cont"&gt;.*?&lt;p class="source"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</span><br><span class="line">    rAuthors = re.compile(<span class="string">r'&lt;div class="cont"&gt;.*?&lt;p class="source"&gt;&lt;a.*?&gt;.*?&lt;a.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</span><br><span class="line">    rContsons = re.compile(<span class="string">r'&lt;div class="cont"&gt;.*?&lt;div class="contson".*?&gt;(.*?)&lt;/div&gt;'</span>, re.S)</span><br><span class="line">    titles = re.findall(rTitles, text)</span><br><span class="line">    dynasties = re.findall(rDynasties, text)</span><br><span class="line">    authors = re.findall(rAuthors, text)</span><br><span class="line">    contsons = re.findall(rContsons, text)</span><br><span class="line">    contents = []</span><br><span class="line">    <span class="keyword">for</span> contson <span class="keyword">in</span> contsons:</span><br><span class="line">        x = re.sub(<span class="string">r'&lt;.*?&gt;'</span>, <span class="string">''</span>, contson)</span><br><span class="line">        contents.append(x)</span><br><span class="line"></span><br><span class="line">    poems = []</span><br><span class="line">    <span class="keyword">for</span> title, dynasty, author, content <span class="keyword">in</span> zip(titles, dynasties, authors, contents):</span><br><span class="line">        poems.append(&#123;</span><br><span class="line">            <span class="string">'title'</span>: title,</span><br><span class="line">            <span class="string">'dynasty'</span>: dynasty,</span><br><span class="line">            <span class="string">'author'</span>: author,</span><br><span class="line">            <span class="string">'content'</span>: content</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    print(poems)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'https://www.gushiwen.org/default_1.aspx'</span></span><br><span class="line">    parsePages(url)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&#39;title&#39;: &#39;满江红·夜雨凉甚忽动从戎之兴&#39;, &#39;dynasty&#39;: &#39;宋代&#39;, &#39;author&#39;: &#39;刘克庄&#39;, &#39;content&#39;: &#39;\n金甲雕戈，记当日、辕门初立。磨盾鼻、一挥千纸，龙蛇犹湿。铁马晓嘶营壁冷，楼船夜渡风涛急。有谁怜、猿臂故将军，无功级。平戎策，从军什。零落尽，慵收拾。把茶经香传，时时温习。生怕客谈榆塞事，且教儿诵花间集。叹臣之壮也不如人，今何及。\n&#39;&#125;, &#123;&#39;title&#39;: &#39;阮郎归·绍兴乙卯大雪行鄱阳道中&#39;, &#39;dynasty&#39;: &#39;宋代&#39;, &#39;author&#39;: &#39;向子諲&#39;, &#39;content&#39;: &#39;\n江南江北雪漫漫，遥知易水寒。同云深处望三关，断肠山又山。天可老，海能翻，消除此恨难。频闻遣使问平安，几时鸾辂还？\n&#39;&#125;, &#123;&#39;title&#39;: &#39;碧瓦&#39;, &#39;dynasty&#39;: &#39;宋代&#39;, &#39;author&#39;: &#39;范成大&#39;, &#39;content&#39;: &#39;\n碧瓦楼头绣幙遮，赤栏桥外绿溪斜。无风杨柳漫天絮，不雨棠梨满地花。\n&#39;&#125;, &#123;&#39;title&#39;: &#39;减字浣溪沙·秋水斜阳演漾金&#39;, &#39;dynasty&#39;: &#39;宋代&#39;, &#39;author&#39;: &#39;贺铸&#39;, &#39;content&#39;: &#39;\n秋水斜阳演漾金，远山隐隐隔平林。几家村落几声砧。记得西楼凝醉眼，昔年风物似如今。只无人与共登临。\n&#39;&#125;, &#123;&#39;title&#39;: &#39;次韵公实雷雨&#39;, &#39;dynasty&#39;: &#39;宋代&#39;, &#39;author&#39;: &#39;洪炎&#39;, &#39;content&#39;: &#39;\n惊雷势欲拔三山，急雨声如倒百川。但作奇寒侵客梦，若为一震静胡烟。田园荆棘漫流水，河洛腥膻今几年。拟叩九关笺帝所，人非大手笔非椽。\n&#39;&#125;, &#123;&#39;title&#39;: &#39;浣溪沙·雨歇梧桐泪乍收&#39;, &#39;dynasty&#39;: &#39;清代&#39;, &#39;author&#39;: &#39;纳兰性德&#39;, &#39;content&#39;: &#39;\n雨歇梧桐泪乍收，遣怀翻自忆从头。摘花销恨旧风流。帘影碧桃人已去，屧痕苍藓径空留。两眉何处月如钩？\n&#39;&#125;, &#123;&#39;title&#39;: &#39;交趾怀古&#39;, &#39;dynasty&#39;: &#39;清代&#39;, &#39;author&#39;: &#39;曹雪芹&#39;, &#39;content&#39;: &#39;\n铜铸金镛振纪纲，声传海外播戎羌。马援自是功劳大，铁笛无烦说子房。\n&#39;&#125;, &#123;&#39;title&#39;: &#39;野老&#39;, &#39;dynasty&#39;: &#39;唐代&#39;, &#39;author&#39;: &#39;杜甫&#39;, &#39;content&#39;: &#39;\n野老篱前江岸回，柴门不正逐江开。渔人网集澄潭下，贾客船随返照来。长路关心悲剑阁，片云何意傍琴台。王师未报收东郡，城阙秋生画角哀。\n&#39;&#125;, &#123;&#39;title&#39;: &#39;碧城三首&#39;, &#39;dynasty&#39;: &#39;唐代&#39;, &#39;author&#39;: &#39;李商隐&#39;, &#39;content&#39;: &#39;\n碧城十二曲阑干，犀辟尘埃玉辟寒。阆苑有书多附鹤，女床无树不栖鸾。星沉海底当窗见，雨过河源隔座看。若是晓珠明又定，一生长对水晶盘。\n对影闻声已可怜，玉池荷叶正田田。不逢萧史休回首，莫见洪崖又拍肩。紫凤放娇衔楚佩，赤鳞狂舞拨湘弦。鄂君怅望舟中夜，绣被焚香独自眠。\n七夕来时先有期，洞房帘箔至今垂。玉轮顾兔初生魄，铁网珊瑚未有枝。检与神方教驻景，收将凤纸写相思。武皇内传分明在，莫道人间总不知。\n&#39;&#125;, &#123;&#39;title&#39;: &#39;水龙吟·听兮清佩琼瑶些&#39;, &#39;dynasty&#39;: &#39;宋代&#39;, &#39;author&#39;: &#39;辛弃疾&#39;, &#39;content&#39;: &#39;\n用“些语”再题瓢泉，歌以饮客，声韵甚谐，客皆为之釂。\n听兮清佩琼瑶些。明兮镜秋毫些。君无去此，流昏涨腻，生蓬蒿些。虎豹甘人，渴而饮汝，宁猿猱些。大而流江海，覆舟如芥，君无助、狂涛些。路险兮山高些。块予独处无聊些。冬槽春盎，归来为我，制松醪些。其外芳芬，团龙片凤，煮云膏些。古人兮既往，嗟予之乐，乐箪瓢些。\n&#39;&#125;]</span><br></pre></td></tr></table></figure><h1>参考</h1><ul><li><a href="https://www.bilibili.com/video/av57661741?p=24&amp;t=0" target="_blank" rel="noopener">python爬虫_从入门到精通（基础篇）http协议、requests库、beautifulsoup库、正则表达式、存取、lxml、xpath、css选择器</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫入门，一步步走上人(F)生(Z)巅(道)峰(路)--进阶2</title>
      <link href="/getting_started_with_crawler_advanced_2.html"/>
      <url>/getting_started_with_crawler_advanced_2.html</url>
      
        <content type="html"><![CDATA[<h1><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="noopener">BeautifulSoup4</a></h1><p>和<code>lxml</code>一样，<code>Beautiful Soup</code>也是一个<code>HTML/XML</code>的解析器，主要功能也是如何解析和提取<code>HTML/XML</code>数据。<code>lxml</code>只会局部遍历，而<code>Beautiful Soup</code>是基于<code>HTML DOM</code>的，会载入整个文档，解析整个<code>DOM</code>树，因此时间和内存开销都会大很多，所以性能要低于<code>lxml</code>。</p><p><code>Beautiful Soup</code>用来解析<code>HTML</code>比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持<code>lxml</code>的<code>XML</code>解析器。</p><a id="more"></a><table><thead><tr><th>解析工具</th><th>解析速度</th><th>使用难度</th></tr></thead><tbody><tr><td>BeautifulSoup4</td><td>最慢</td><td>最简单</td></tr><tr><td>lxml</td><td>快</td><td>简单</td></tr><tr><td>正则</td><td>最快</td><td>最难</td></tr></tbody></table><h2 id="find与find-all方法"><a class="header-anchor" href="#find与find-all方法"></a><code>find</code>与<code>find_all</code>方法</h2><h3 id="简单使用"><a class="header-anchor" href="#简单使用"></a>简单使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br></pre></td></tr></table></figure><h4 id="获取所有tr标签"><a class="header-anchor" href="#获取所有tr标签"></a>获取所有<code>tr</code>标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trs = soup.find_all(<span class="string">'tr'</span>)</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    print(tr)</span><br></pre></td></tr></table></figure><h4 id="获取第二个tr标签"><a class="header-anchor" href="#获取第二个tr标签"></a>获取第二个<code>tr</code>标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tr = soup.find_all(<span class="string">'tr'</span>, limit=<span class="number">2</span>)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="获取所有class等于even的tr标签"><a class="header-anchor" href="#获取所有class等于even的tr标签"></a>获取所有<code>class</code>等于<code>even</code>的<code>tr</code>标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trs = soup.find_all('tr', class_='even')</span></span><br><span class="line">trs = soup.find_all(<span class="string">'tr'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'even'</span>&#125;)</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    print(tr)</span><br></pre></td></tr></table></figure><h4 id="将所有id等于test，class等于test的a标签提取出来"><a class="header-anchor" href="#将所有id等于test，class等于test的a标签提取出来"></a>将所有<code>id</code>等于<code>test</code>，<code>class</code>等于<code>test</code>的<code>a</code>标签提取出来</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">aList = soup.find_all(<span class="string">'a'</span>, id=<span class="string">'test'</span>, class_=<span class="string">'test'</span>)</span><br><span class="line">aList = soup.find_all(<span class="string">'a'</span>, attrs=&#123;<span class="string">'id'</span>: <span class="string">'test'</span>, <span class="string">'class'</span>: <span class="string">'test'</span>&#125;)</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> aList:</span><br><span class="line">    print(a)</span><br></pre></td></tr></table></figure><h4 id="获取所有a标签的href属性"><a class="header-anchor" href="#获取所有a标签的href属性"></a>获取所有<code>a</code>标签的<code>href</code>属性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">aList = soup.find_all(<span class="string">'a'</span>)</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> aList:</span><br><span class="line">    <span class="comment"># 1. 通过下标获取</span></span><br><span class="line">    <span class="comment"># href = a['href']</span></span><br><span class="line">    <span class="comment"># 2. 通过attrs属性获取</span></span><br><span class="line">    href = a.attrs[<span class="string">'href'</span>]</span><br><span class="line">    print(href)</span><br></pre></td></tr></table></figure><h4 id="获取所有职位信息"><a class="header-anchor" href="#获取所有职位信息"></a>获取所有职位信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">trs = soup.find_all(<span class="string">'tr'</span>)[<span class="number">1</span>:]</span><br><span class="line">movies = []</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    <span class="comment"># 方式1</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    tds = tf.find_all('td')</span></span><br><span class="line"><span class="string">    title = tds[0].string</span></span><br><span class="line"><span class="string">    category = tds[1].string</span></span><br><span class="line"><span class="string">    nums = tds[2].string</span></span><br><span class="line"><span class="string">    city = tds[3].string</span></span><br><span class="line"><span class="string">    pubtime = tds[4].string</span></span><br><span class="line"><span class="string">    movie = &#123;</span></span><br><span class="line"><span class="string">        'title': title, </span></span><br><span class="line"><span class="string">        'category': category, </span></span><br><span class="line"><span class="string">        'nums': nums, </span></span><br><span class="line"><span class="string">        'city': city,</span></span><br><span class="line"><span class="string">        'pubtime': pubtime</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 方式2</span></span><br><span class="line">    infos = list(tr.stripped_strings)</span><br><span class="line">    movie = &#123;</span><br><span class="line">        <span class="string">'title'</span>: infos[<span class="number">0</span>], </span><br><span class="line">        <span class="string">'category'</span>: infos[<span class="number">1</span>], </span><br><span class="line">        <span class="string">'nums'</span>: infos[<span class="number">2</span>], </span><br><span class="line">        <span class="string">'city'</span>: infos[<span class="number">3</span>],</span><br><span class="line">        <span class="string">'pubtime'</span>: infos[<span class="number">4</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    movies.append(movie)</span><br></pre></td></tr></table></figure><h3 id="笔记"><a class="header-anchor" href="#笔记"></a>笔记</h3><h4 id="find-all的使用"><a class="header-anchor" href="#find-all的使用"></a><code>find_all</code>的使用</h4><ol><li>在提取标签时，第一个参数是标签的名字。如果在提取标签的时候想要使用标签属性进行过滤，那么可以在这个方法中通过关键字参数的形式，将属性的名字以及对应的值传进去。或者使用<code>attrs</code>属性，将所有的属性以及对应的值放在一个字典中传给<code>attrs</code>。</li><li>有时在提取标签的时候，不想提取那么多，可以使用<code>limit</code>参数限制提取多少个。</li></ol><h4 id="find与find-all的区别"><a class="header-anchor" href="#find与find-all的区别"></a><code>find</code>与<code>find_all</code>的区别</h4><ul><li><code>find</code>:找到第一个满足条件的标签就返回，即只会返回一个元素。</li><li><code>find_all</code>:将所有满足条件的标签都返回。</li></ul><h4 id="使用find和find-all的过滤条件"><a class="header-anchor" href="#使用find和find-all的过滤条件"></a>使用<code>find</code>和<code>find_all</code>的过滤条件</h4><ol><li>关键字参数：将属性的名字作为关键字参数的名字，以及属性的值作为关键字参数的值进行过滤</li><li><code>attrs</code>参数：将属性条件放在一个字典中。</li></ol><h4 id="获取标签的属性"><a class="header-anchor" href="#获取标签的属性"></a>获取标签的属性</h4><ol><li><p>通过下表获取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">href = a[<span class="string">'href'</span>]</span><br></pre></td></tr></table></figure></li><li><p>通过<code>attrs</code>属性获取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">href = a.attrs[<span class="string">'href'</span>]</span><br></pre></td></tr></table></figure></li></ol><h4 id="string，strings，stripped-strings属性和get-text方法"><a class="header-anchor" href="#string，strings，stripped-strings属性和get-text方法"></a><code>string</code>，<code>strings</code>，<code>stripped_strings</code>属性和<code>get_text</code>方法</h4><ul><li><code>string</code>:获取某个标签下的非标签字符串，返回一个字符串。如果这个标签下有多行字符，就无法获取。</li><li><code>strings</code>:获取某个标签下的子孙非标签字符串，返回一个生成器。</li><li><code>stripped_strings</code>:获取某个标签下的子孙非标签字符串，会去掉空白字符，返回一个生成器。</li><li><code>get_text</code>：获取某个标签下的子孙非标签字符串，不是以列表的形式返回，返回普通字符串</li></ul><h2 id="select方法"><a class="header-anchor" href="#select方法"></a><code>select</code>方法</h2><h3 id="CSS选择器"><a class="header-anchor" href="#CSS选择器"></a><code>CSS</code>选择器</h3><p><code>html</code>代码如下，通过更改<code>head</code>标签中的<code>style</code>标签进行选择。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span></span><br><span class="line"><span class="css"><span class="selector-tag">div</span><span class="selector-class">.line1</span>&#123;</span></span><br><span class="line">background-color: pink; </span><br><span class="line">&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"box"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>第零行数据<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"line1"</span>&gt;</span>第一行数据<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>第二行数据<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">id</span>=<span class="string">"line3"</span>&gt;</span>第三行数据<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"line1"</span>&gt;</span>div 中的数据<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"username"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"password"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><ol><li><p>根据标签的名字选择</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">p</span>&#123;</span><br><span class="line">    <span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>根据类型选择，在类的前面加一个点</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.line</span>&#123;</span><br><span class="line"><span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>根据<code>id</code>名字选择，在<code>id</code>前面加<code>#</code>号</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#box</span>&#123;</span><br><span class="line">    <span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>查找子孙元素，需要在子孙元素中间有一个空格</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#box</span> <span class="selector-tag">p</span>&#123;</span><br><span class="line">    <span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>查找直接子元素，需要在父子元素中间有个<code>&gt;</code></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#box</span> &gt; <span class="selector-tag">p</span>&#123;</span><br><span class="line">    <span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>根据属性的名字进行查找，则应该先写标签名字，然后在中括号中写属性的值</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">input</span><span class="selector-attr">[name=<span class="string">'username'</span>]</span>&#123;</span><br><span class="line">    <span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在根据类名或<code>id</code>进行查找时，如果还要根据标签名进行过滤，那么可以在类的前面或者<code>id</code>的前面加上标签名字</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span><span class="selector-id">#line1</span>&#123;</span><br><span class="line">    <span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">div</span><span class="selector-class">.line1</span>&#123;</span><br><span class="line">    <span class="attribute">background-color</span>: pink;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="select和css选择器"><a class="header-anchor" href="#select和css选择器"></a><code>select</code>和<code>css</code>选择器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">html = <span class="string">''</span></span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br></pre></td></tr></table></figure><h4 id="获取所有tr标签-v2"><a class="header-anchor" href="#获取所有tr标签-v2"></a>获取所有<code>tr</code>标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trs = soup.select(<span class="string">'tr'</span>)</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    print(tr)</span><br></pre></td></tr></table></figure><h4 id="获取第二个tr标签-v2"><a class="header-anchor" href="#获取第二个tr标签-v2"></a>获取第二个<code>tr</code>标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tr = soup.select(<span class="string">'tr'</span>)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="获取所有class等于even的tr标签-v2"><a class="header-anchor" href="#获取所有class等于even的tr标签-v2"></a>获取所有<code>class</code>等于<code>even</code>的<code>tr</code>标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trs = soup.select('tr.even')</span></span><br><span class="line">trs = soup.select(<span class="string">"tr[class='even']"</span>)</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    print(tr)</span><br></pre></td></tr></table></figure><h4 id="获取所有a标签的href属性-v2"><a class="header-anchor" href="#获取所有a标签的href属性-v2"></a>获取所有<code>a</code>标签的<code>href</code>属性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">aList = soup.select(<span class="string">'a'</span>)</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> aList:</span><br><span class="line">    href = a[<span class="string">'href'</span>]</span><br><span class="line">    print(href)</span><br></pre></td></tr></table></figure><h4 id="获取所有职位信息-v2"><a class="header-anchor" href="#获取所有职位信息-v2"></a>获取所有职位信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">trs = soup.select(<span class="string">'tr'</span>)[<span class="number">1</span>:]</span><br><span class="line">movies = []</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs：</span><br><span class="line">    infos = list(tr.stripped_strings)</span><br><span class="line">    movie = &#123;</span><br><span class="line">        <span class="string">'title'</span>: infos[<span class="number">0</span>], </span><br><span class="line">        <span class="string">'category'</span>: infos[<span class="number">1</span>], </span><br><span class="line">        <span class="string">'nums'</span>: infos[<span class="number">2</span>], </span><br><span class="line">        <span class="string">'city'</span>: infos[<span class="number">3</span>],</span><br><span class="line">        <span class="string">'pubtime'</span>: infos[<span class="number">4</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    movies.append(movie)</span><br></pre></td></tr></table></figure><h2 id="补充"><a class="header-anchor" href="#补充"></a>补充</h2><h3 id="常见的四种对象"><a class="header-anchor" href="#常见的四种对象"></a>常见的四种对象</h3><ul><li><code>Tag</code>: <code>BeautifulSoup</code>中的所有标签都是<code>Tag</code>类型，<code>BeautifulSoup</code>继承于<code>Tag</code></li><li><code>BeautifulSoup</code>：继承自<code>Tag</code>,用来生成<code>BeautifulSoup</code>树</li><li><code>NavigableString</code>:继承自Python中的<code>str</code>，用起来和<code>str</code>一样</li><li><code>Comment</code>:继承自<code>NavigableString</code></li></ul><h3 id="contents和children"><a class="header-anchor" href="#contents和children"></a><code>contents</code>和<code>children</code></h3><p>返回某个标签下的直接子元素，其中也包括字符串。二者的区别是：<code>contents</code>返回一个列表，<code>children</code>返回一个迭代器。</p><h1>参考</h1><ul><li><a href="https://www.bilibili.com/video/BV1Lx41197NM?p=15" target="_blank" rel="noopener">python爬虫_从入门到精通（基础篇）http协议、requests库、beautifulsoup库、正则表达式、存取、lxml、xpath、css选择器</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫入门，一步步走上人(F)生(Z)巅(道)峰(路)--进阶1</title>
      <link href="/getting_started_with_crawler_advanced_1.html"/>
      <url>/getting_started_with_crawler_advanced_1.html</url>
      
        <content type="html"><![CDATA[<h1><a href="http://2.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="noopener">Requests库</a></h1><p><code>Requests</code> :唯一的一个<strong>非转基因</strong>的 Python HTTP 库，人类可以安全享用。</p><p><strong>警告</strong>：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。</p><a id="more"></a><h2 id="发送get请求"><a class="header-anchor" href="#发送get请求"></a>发送get请求</h2><p>直接调用<code>requests.get </code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br></pre></td></tr></table></figure><h2 id="response的属性"><a class="header-anchor" href="#response的属性"></a>response的属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">params = &#123;<span class="string">'wd'</span>: <span class="string">'python'</span>&#125;</span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4090.0 Safari/537.36 Edg/83.0.467.0'</span>&#125;</span><br><span class="line"><span class="comment"># params: 接受一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()</span></span><br><span class="line">response = requests.get(<span class="string">'https://www.baidu.com'</span>, params=params, headers=headers)</span><br><span class="line"><span class="comment"># 查看响应内容，response.text返回的是Unicode格式的数据</span></span><br><span class="line">print(response.text)</span><br><span class="line"><span class="comment"># 查看响应内容，response.content返回的是字节流数据</span></span><br><span class="line">print(response.content.decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="comment"># 查看完整url地址</span></span><br><span class="line">print(response.url)</span><br><span class="line"><span class="comment"># 查看响应头部字符编码</span></span><br><span class="line">print(response.encoding)</span><br><span class="line"><span class="comment"># 查看响应码</span></span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure><h2 id="response-text和response-content的区别"><a class="header-anchor" href="#response-text和response-content的区别"></a>response.text和response.content的区别</h2><ul><li><code>response.content</code>:直接从网络上面抓取的数据，没有经过任何解码，所以是<code>bytes</code>类型（硬盘上和网络上传输的字符串都是<code>bytes</code>类型）</li><li><code>response.text</code>:将<code>response.content</code>进行解码的字符串，数据类型为<code>str</code>,解码需要指定一个编码方式，<code>requests</code>会根据自己的猜测来判断编码的方式，所以有时会猜测错误，就会导致解码产生乱码。这时候应该使用<code>response.content.decode('utf-8')</code>进行手动解码</li></ul><h2 id="发送POST请求"><a class="header-anchor" href="#发送POST请求"></a>发送POST请求</h2><p>直接调用<code>requests.post</code>，如果返回的是<code>json</code>数据，可以调用<code>response.json()</code>来将<code>json</code>字符串转为字典或列表</p><p>下面是爬取拉勾网的一个示例，记得请求头添加<code>Cookie</code>，才能成功爬取到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">'first'</span>: <span class="string">'true'</span>,</span><br><span class="line">        <span class="string">'pn'</span>: <span class="string">'1'</span>,</span><br><span class="line">        <span class="string">'kd'</span>: <span class="string">'python'</span>&#125;</span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '</span></span><br><span class="line">                         <span class="string">'Chrome/83.0.4090.0 Safari/537.36 Edg/83.0.467.0'</span>,</span><br><span class="line">           <span class="string">'Referer'</span>: <span class="string">'https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput='</span>,</span><br><span class="line">           <span class="string">'Cookie'</span>: <span class="string">'user_trace_token=20200331183800-0c1f510a-ae9a-4f04-b70d-e17f9edec031; '</span></span><br><span class="line">                     <span class="string">'LGUID=20200331183800-b8eca414-b7b2-479d-8100-71fff41d8087; _ga=GA1.2.17010052.1585651081; '</span></span><br><span class="line">                     <span class="string">'index_location_city=%E5%85%A8%E5%9B%BD; lagou_utm_source=B; _gid=GA1.2.807051168.1585805257; '</span></span><br><span class="line">                     <span class="string">'sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%22171302b7caa67e-0dedc0121b2532-255e0c45'</span></span><br><span class="line">                     <span class="string">'-2073600-171302b7cabb9c%22%2C%22%24device_id%22%3A%22171302b7caa67e-0dedc0121b2532-255e0c45'</span></span><br><span class="line">                     <span class="string">'-2073600-171302b7cabb9c%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B'</span></span><br><span class="line">                     <span class="string">'%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22'</span></span><br><span class="line">                     <span class="string">'%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5'</span></span><br><span class="line">                     <span class="string">'%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%7D; '</span></span><br><span class="line">                     <span class="string">'JSESSIONID=ABAAABAABFIAAAC7D7CECCAFCFFA1FCBF3CB10D8EA6A189; '</span></span><br><span class="line">                     <span class="string">'WEBTJ-ID=20200403095906-1713dc35e58a75-0b564b9cba1732-23580c45-2073600-1713dc35e598e; PRE_UTM=; '</span></span><br><span class="line">                     <span class="string">'PRE_HOST=; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2F; '</span></span><br><span class="line">                     <span class="string">'LGSID=20200403095905-8201da05-4bb8-4e93-97bf-724ea6f758af; '</span></span><br><span class="line">                     <span class="string">'PRE_SITE=https%3A%2F%2Fwww.lagou.com; _gat=1; '</span></span><br><span class="line">                     <span class="string">'Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1585651082,1585879146; TG-TRACK-CODE=index_search; '</span></span><br><span class="line">                     <span class="string">'X_HTTP_TOKEN=0b356dc3463713117419785851e40fa7a09468f3f0; '</span></span><br><span class="line">                     <span class="string">'Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1585879149; '</span></span><br><span class="line">                     <span class="string">'LGRID=20200403095908-4f1711b9-3e7e-4d54-a400-20c76b57f327; '</span></span><br><span class="line">                     <span class="string">'SEARCH_ID=b875e8b91a764d63a2dc98d822ca1f85'</span>&#125;</span><br><span class="line">response = requests.post(<span class="string">'https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false'</span>,</span><br><span class="line">                         headers=headers,</span><br><span class="line">                         data=data)</span><br><span class="line">print(response.json())</span><br></pre></td></tr></table></figure><h2 id="使用代理ip"><a class="header-anchor" href="#使用代理ip"></a>使用代理ip</h2><p>这里在<a href="https://darlewo.cn/2020/04/02/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%EF%BC%8C%E4%B8%80%E6%AD%A5%E6%AD%A5%E8%B5%B0%E4%B8%8A%E4%BA%BA-F-%E7%94%9F-Z-%E5%B7%85-%E9%81%93-%E5%B3%B0-%E8%B7%AF-%E5%9F%BA%E7%A1%80/">基础</a>中已经讲到过，使用<code>requests</code>只需要两行代码，非常方便</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxy = &#123;<span class="string">'http'</span>: <span class="string">'59.44.78.30:54069'</span>&#125;</span><br><span class="line">response = requests.get(<span class="string">'http://httpbin.org/ip'</span>, proxies=proxy)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><h2 id="cookie"><a class="header-anchor" href="#cookie"></a>cookie</h2><p>使用<code>session</code>在多次请求中共享<code>cookie</code>，可以发现相比使用<code>urllib</code>代码变得特别简洁</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">''</span>&#125;</span><br><span class="line">data = &#123;<span class="string">'email'</span>: <span class="string">''</span>,</span><br><span class="line">        <span class="string">'password'</span>: <span class="string">''</span>&#125;</span><br><span class="line">login_url = <span class="string">'http://www.renren.com/PLogin.do'</span></span><br><span class="line">profile_url = <span class="string">'http://www.renren.com/880151247/profile'</span></span><br><span class="line">session = requests.Session()</span><br><span class="line">session.post(login_url, data=data, headers=headers)</span><br><span class="line">response = session.get(profile_url)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'renren.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.text)</span><br></pre></td></tr></table></figure><h2 id="处理不信任的SSL证书"><a class="header-anchor" href="#处理不信任的SSL证书"></a>处理不信任的SSL证书</h2><p>对于那些没有被信任的<code>SSL</code>证书的网站，可以在<code>requests.get</code>和<code>requests.post</code>中设置参数<code>verify=False</code>来进行访问</p><h1>XPath语法和lxml模块</h1><h2 id="XPath"><a class="header-anchor" href="#XPath"></a>XPath</h2><p><code>xpath(XML Path Language)</code>是一门在<code>XML</code>和<code>HTML</code>文档中查找信息的语言，可用来在<code>XML</code>和<code>HTML</code>文档中对元素和属性进行访问。</p><h2 id="XPath开发工具"><a class="header-anchor" href="#XPath开发工具"></a>XPath开发工具</h2><ul><li>Chrome插件XPath Helper</li><li>Firefox插件Xpath Checker</li></ul><h2 id="XPath语法"><a class="header-anchor" href="#XPath语法"></a>XPath语法</h2><p><code>XPath</code>使用路径表达式来选取<code>XML</code>文档中的节点或者节点集，这些路径表达式和我们在常规的电脑文件系统中的表示式非常类似。</p><table><thead><tr><th>表达式</th><th style="text-align:left">描述</th><th style="text-align:left">示例</th><th>结果</th></tr></thead><tbody><tr><td>nodename</td><td style="text-align:left">选取此节点的所有子节点</td><td style="text-align:left">bookstore</td><td>选取bookstore下所有的子节点</td></tr><tr><td>/</td><td style="text-align:left">如果在最前面，代表从根节点选取，否则选择某节点下的某个节点</td><td style="text-align:left">/bookstore</td><td>选取根元素下所有的bookstore节点</td></tr><tr><td>//</td><td style="text-align:left">从全局节点中选择节点，随便在哪个位置</td><td style="text-align:left">//book</td><td>从全局节点中找到所有的book节点</td></tr><tr><td>@</td><td style="text-align:left">选取某个节点的属性</td><td style="text-align:left">//book[@price]</td><td>选择所有拥有price属性的book节点</td></tr></tbody></table><h3 id="谓语"><a class="header-anchor" href="#谓语"></a>谓语</h3><p>谓语用来查找某个特定的节点或者包含某个指定节点的值的节点，被嵌在方括号中。</p><table><thead><tr><th>路径表达式</th><th>描述</th></tr></thead><tbody><tr><td>/bookstore/book[1]</td><td>选取bookstore下的第一个book元素</td></tr><tr><td>/booksotre/book[last()]</td><td>选取bookstore下的最后一个book元素</td></tr><tr><td>/bookstore/book[position()❤️]</td><td>选取bookstore下前面两个book元素</td></tr><tr><td>//book[@price]</td><td>选择所有拥有price属性的book节点</td></tr><tr><td>//book[@price=10]</td><td>选取所有属性price=10的book元素</td></tr></tbody></table><h3 id="通配符"><a class="header-anchor" href="#通配符"></a>通配符</h3><table><thead><tr><th>通配符</th><th>描述</th><th>示例</th><th>结果</th></tr></thead><tbody><tr><td>*</td><td>匹配任意节点</td><td>/bookstore/*</td><td>选取bookstore下的所有子元素</td></tr><tr><td>@*</td><td>匹配节点中的任何属性</td><td>//book[@*]</td><td>选取所有带有属性的book元素</td></tr></tbody></table><h3 id="选取多个路径"><a class="header-anchor" href="#选取多个路径"></a>选取多个路径</h3><p>通过在路径表达式中使用<code>|</code>运算符，可以选取若干个路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;bookstore&#x2F;book | &#x2F;&#x2F;book&#x2F;title</span><br><span class="line"># 选取多个book元素以及book元素下的title元素</span><br></pre></td></tr></table></figure><h3 id="运算符"><a class="header-anchor" href="#运算符"></a>运算符</h3><table><thead><tr><th>运算符</th><th>描述</th><th>实例</th><th>返回值</th></tr></thead><tbody><tr><td>|</td><td>计算两个节点集</td><td>//book | //cd</td><td>返回所有拥有book和cd元素的节点集</td></tr><tr><td>+，-，*，div</td><td>加，减，乘，除</td><td>6+1， 6-1， 6 * 1， 6 div 1</td><td>7, 5, 6, 6</td></tr><tr><td>=, !=, &lt;, &lt;=, &gt;, &gt;=</td><td>-</td><td>-</td><td>返回false或true</td></tr><tr><td>or, and</td><td>或，与</td><td>-</td><td>返回false或true</td></tr><tr><td>mod</td><td>计算除法的余数</td><td>5 mod 2</td><td>1</td></tr></tbody></table><h3 id="注意事项"><a class="header-anchor" href="#注意事项"></a>注意事项</h3><ol><li><p><code>/</code>和<code>//</code>的区别，<code>/</code>代表只获取直接子节点，<code>//</code>代表获取子孙节点。</p></li><li><p><code>contains:</code>有时候某个属性中包含了多个值，那么可以使用<code>contains</code>函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;div[contains(@class, &#39;job_detail&#39;)]</span><br></pre></td></tr></table></figure></li><li><p>谓词中的下标从1开始。</p></li></ol><h2 id="lxml库"><a class="header-anchor" href="#lxml库"></a>lxml库</h2><p><a href="https://lxml.de/" target="_blank" rel="noopener"><code>lxml</code></a>是一个<code>HTML/XML</code>的解析器，主要功能是如何解析和提取<code>HTML/XML</code>数据。</p><h3 id="基本使用"><a class="header-anchor" href="#基本使用"></a>基本使用</h3><ol><li><p>解析html字符串：使用<code>lxml.etree.HTML</code>进行解析</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">htmlElement = etree.HTML(text)</span><br><span class="line">print(etree.tostring(htmlElement, encoding=<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></li><li><p>解析html文件：使用<code>lxml.etree.parse</code>进行解析</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">htmlElement = etree.parse(<span class="string">'tencent.xml'</span>)</span><br><span class="line">print(etree.tostring(htmlElement, encoding=<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure><p>这个函数默认使用<code>XML</code>解析器，所以如果碰到不规范的<code>HTML</code>代码的时候就会解析错误，这时候要创建<code>HTML</code>解析器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">parser = etree.HTMLParser(encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">htmlElement = etree.parse(<span class="string">'tencent.xml'</span>, parser=parser)</span><br><span class="line">print(etree.tostring(htmlElement, encoding=<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></li></ol><h2 id="XPath和lxml结合使用"><a class="header-anchor" href="#XPath和lxml结合使用"></a>XPath和lxml结合使用</h2><ol><li>使用<code>xpath</code>语法，应该使用<code>Element.xpath</code>方法，<code>xpath</code>返回列表。</li><li>获取标签属性</li><li>获取文本使用<code>xpath</code>中的<code>text()</code>函数</li><li>如果想在某个标签下，再执行<code>xpath</code>，获取这个标签下的子孙元素，那么应该在斜杠之前加点，代表在当前元素下获取。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">parser = etree.HTMLParser(encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">html = etree.parse(<span class="string">'tencent.html'</span>, parser=parser)</span><br></pre></td></tr></table></figure><h3 id="获取所有tr标签"><a class="header-anchor" href="#获取所有tr标签"></a>获取所有tr标签</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trs = html.xpath(<span class="string">'//tr'</span>)</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    print(etree.tostring(tr, encoding=<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure><h3 id="获取第2个tr标签"><a class="header-anchor" href="#获取第2个tr标签"></a>获取第2个tr标签</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr = html.xpath(<span class="string">'//tr[2]'</span>)[<span class="number">0</span>]</span><br><span class="line">print(etree.tostring(tr, encoding=<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure><h3 id="获取所有class等于even的tr标签"><a class="header-anchor" href="#获取所有class等于even的tr标签"></a>获取所有class等于even的tr标签</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trs = html.xpath(<span class="string">"//tr[@class='even']"</span>)</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    print(etree.tostring(tr, encoding=<span class="string">'utf-8'</span>).decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure><h3 id="获取所有a标签的href属性"><a class="header-anchor" href="#获取所有a标签的href属性"></a>获取所有a标签的href属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">aList = html.xpath(<span class="string">'//a/@href'</span>)</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> aList:</span><br><span class="line">    print(<span class="string">'http://hr.tencent.com/'</span> + a)</span><br></pre></td></tr></table></figure><h3 id="获取所有职位信息"><a class="header-anchor" href="#获取所有职位信息"></a>获取所有职位信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">trs = html.xpath(<span class="string">'//tr[position()&gt;1]'</span>)</span><br><span class="line">positions = []</span><br><span class="line"><span class="keyword">for</span> tr <span class="keyword">in</span> trs:</span><br><span class="line">    href = tr.xpath(<span class="string">'.//a/@href'</span>)[<span class="number">0</span>]</span><br><span class="line">    fullurl = <span class="string">'http://hr.tencent.com/'</span> + href</span><br><span class="line">    title = tr.xpath(<span class="string">'./td[1]//text()'</span>)[<span class="number">0</span>]</span><br><span class="line">    category = tr.xpath(<span class="string">'./td[2]//text()'</span>)[<span class="number">0</span>]</span><br><span class="line">    nums = tr.xpath(<span class="string">'./td[3]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">    address = tr.xpath(<span class="string">'./td[4]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">    pubtime = tr.xpath(<span class="string">'./td[5]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    position = &#123;</span><br><span class="line">        <span class="string">'url'</span>: fullurl,</span><br><span class="line">        <span class="string">'title'</span>: title,</span><br><span class="line">        <span class="string">'category'</span>: category,</span><br><span class="line">        <span class="string">'nums'</span>: nums,</span><br><span class="line">        <span class="string">'address'</span>: address,</span><br><span class="line">        <span class="string">'pubtime'</span>: pubtime</span><br><span class="line">    &#125;</span><br><span class="line">    positions.append(position)</span><br></pre></td></tr></table></figure><h1>参考</h1><ul><li><a href="https://www.bilibili.com/video/BV1Lx41197NM?p=15" target="_blank" rel="noopener">python爬虫_从入门到精通（基础篇）http协议、requests库、beautifulsoup库、正则表达式、存取、lxml、xpath、css选择器</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫入门，一步步走上人(F)生(Z)巅(道)峰(路)--基础</title>
      <link href="/getting_started_with_crawler_basis.html"/>
      <url>/getting_started_with_crawler_basis.html</url>
      
        <content type="html"><![CDATA[<h1>开发环境</h1><p>python3 + Microsoft Edge Chromium</p><h1>爬虫入门知识</h1><h2 id="HTTP请求过程"><a class="header-anchor" href="#HTTP请求过程"></a>HTTP请求过程</h2><ol><li>当用户在浏览器的地址栏中输入一个URL并按回车之后，浏览器会向HTTP服务器发送Request请求，去获取该URL的html文件，服务器把Response文件对象发送回浏览器。HTTP请求主要分为<code>Get</code>和<code>Post</code>两种方法。</li><li>浏览器分析Reponse中的html，发现其中引用了很多其他文件，比如images文件，css文件，JS文件。浏览器会自动再次发送Request去获取这些文件。</li><li>当所有的文件都下载成功后，网页会根据HTML语法结构完整的显示出来。</li></ol><a id="more"></a><h2 id="常用请求方法"><a class="header-anchor" href="#常用请求方法"></a>常用请求方法</h2><ol><li><code>Get</code>请求 ：只从服务器获取数据，不会对服务器资源产生影响</li><li><code>Post</code>请求：向服务器发送数据（登录），上传文件等，会对服务器资源产生影响。</li></ol><h2 id="请求头常见参数"><a class="header-anchor" href="#请求头常见参数"></a>请求头常见参数</h2><ol><li><code>User-Agent</code>:浏览器名称，如果我们使用爬虫发送请求，那么我们的<code>User-Agent</code>就是<code>Python</code>,很容易被反爬虫检测到；</li><li><code>Referer</code>:表明当前请求是从哪个URL过来的，这个一般也可以用来做反爬虫技术。如果不是从指定页面过来的，那么就不做相关的响应。</li><li><code>Cookie</code>:<code>http</code>协议视无状态的，也就是从一个人发送了两次请求，服务器没有能力知道这两个请求是否来自同一个人。因此这时候就用<code>cookie</code>来做标识。一般如果想要做登录后才能访问的网站，那么就需要发送<code>cookie</code>信息了。</li></ol><h2 id="常见响应状态码"><a class="header-anchor" href="#常见响应状态码"></a>常见响应状态码</h2><ol><li><code>200</code>:请求正常，服务器正常返回数据；</li><li><code>301</code>:永久重定向，比如在访问<code>www.jingdong.com</code>的时候回重定向到<code>www.jd.com</code>；</li><li><code>302</code>:临时重定向，比如在访问一个需要登录的页面的时候，而此时没有登录，那么就会重定向到登录界面；</li><li><code>400</code>:请求的<code>url</code>在服务器上找不到，换句话说就是请求<code>url</code>错误；</li><li><code>403</code>:服务器拒绝访问，权限不够；</li><li><code>500</code>:服务器内部错误，可能是服务器出现<code>bug</code>了。</li></ol><h1>urllib库</h1><h2 id="urlopen"><a class="header-anchor" href="#urlopen"></a>urlopen</h2><p>一个简单的读取百度首页的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">resp = request.urlopen(<span class="string">'https://baidu.com'</span>)</span><br><span class="line">print(resp.read())</span><br></pre></td></tr></table></figure><p>返回值<code>resp</code>有多个方法<code>read</code>,<code>readline</code>,<code>readlines</code>,<code>getcode</code></p><h2 id="urlretrieve"><a class="header-anchor" href="#urlretrieve"></a>urlretrieve</h2><p>下载网页</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">request.urlretrieve(<span class="string">'https://www.baidu.com'</span>, <span class="string">'baidu.html'</span>)</span><br></pre></td></tr></table></figure><h2 id="urlencode"><a class="header-anchor" href="#urlencode"></a>urlencode</h2><p>用浏览器发送请求的时候，如果<code>url</code>中包含了中文或者其他特殊字符，那么浏览器会自动进行编码。如果使用代码发送请求，则必须手动进行编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'zzr'</span>, <span class="string">'age'</span>: <span class="number">18</span>&#125;</span><br><span class="line">qs = parse.urlencode(data)</span><br><span class="line">print(qs)</span><br></pre></td></tr></table></figure><h2 id="parse-qs"><a class="header-anchor" href="#parse-qs"></a>parse_qs</h2><p>将经过编码后的<code>url</code>参数进行解码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line">qs = <span class="string">'name=zzr&amp;age=18'</span></span><br><span class="line">print(parse.parse_qs(qs))</span><br></pre></td></tr></table></figure><h2 id="urlparse和urlsplit"><a class="header-anchor" href="#urlparse和urlsplit"></a>urlparse和urlsplit</h2><p>有时候拿到一个<code>url</code>，想要对这个<code>url</code>中的各个组成部分进行分割</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd=python&amp;rsv_spt=1'</span></span><br><span class="line">print(parse.urlparse(url))</span><br><span class="line">print(parse.urlsplit(url))</span><br></pre></td></tr></table></figure><p>两个函数基本一样，唯一不同的地方是<code>urlparse</code>多了一个<code>params</code>属性</p><h2 id="request-Request类"><a class="header-anchor" href="#request-Request类"></a>request.Request类</h2><p>如果想在请求的时候增加一些请求头，则必须使用<code>Request</code>类来实现，下面是爬取拉勾网职位信息的一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse, request</span><br><span class="line">url = <span class="string">'https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false'</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4090.0 Safari/537.36 Edg/83.0.467.0'</span>,</span><br><span class="line">           <span class="string">'Referer'</span>: <span class="string">'https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput='</span>,</span><br><span class="line">data = &#123;<span class="string">'first'</span>: <span class="string">'true'</span>,</span><br><span class="line">        <span class="string">'pn'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'kd'</span>: <span class="string">'python'</span>&#125;</span><br><span class="line">req = request.Request(url, headers=headers,</span><br><span class="line">                      data=parse.urlencode(data).encode(<span class="string">'utf-8'</span>),</span><br><span class="line">                      method=<span class="string">'POST'</span>)</span><br><span class="line">resp = request.urlopen(req)</span><br><span class="line">print(resp.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_Ao39Ot2brL.png?x-oss-process=style/ossProcess" alt=""></p><p>对应的请求头参数在上面网页按<code>F12</code>进行查找。但上面的代码已经读取不到拉勾网的职位信息，因为他们的反爬虫技术也在进步。但是记住一点，在使用<code>Request</code>类的时候，一定记得加上请求头<code>headers</code>，要不然非常容易被反爬到。</p><h2 id="ProxyHandler处理器（代理设置）"><a class="header-anchor" href="#ProxyHandler处理器（代理设置）"></a>ProxyHandler处理器（代理设置）</h2><p>很多网站会检测某一段时间某个IP的访问次数（通过流量，系统日志等），如果访问次数太多，它会禁止这个IP的访问，所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算IP被禁止，依然可以换个IP继续爬取。</p><h3 id="代理的原理："><a class="header-anchor" href="#代理的原理："></a>代理的原理：</h3><p>在请求目的网站之前，先请求代理服务器，然后让代理服务器去请求目的网站，然后代理服务器再将拿到的数据转发给我们的代码。</p><p>常用代理有：</p><ul><li><a href="https://www.xicidaili.com/" target="_blank" rel="noopener">西刺免费代理</a></li><li><a href="https://www.kuaidaili.com/" target="_blank" rel="noopener">快代理</a></li><li><a href="http://www.dailiyun.com/" target="_blank" rel="noopener">代理云</a></li></ul><p>我们这里通过快代理获取免费的ip</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_uFeiY1keA1.png?x-oss-process=style/ossProcess" alt=""></p><p><code>urllib</code>中通过<code>ProxyHandler</code>来设置使用代理服务器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">url = <span class="string">'http://httpbin.org/ip'</span></span><br><span class="line"><span class="comment"># 没有使用代理</span></span><br><span class="line"><span class="comment"># resp = request.urlopen(url)</span></span><br><span class="line"><span class="comment"># print(resp.read())</span></span><br><span class="line"><span class="comment"># 使用代理</span></span><br><span class="line"><span class="comment"># 1. 使用ProxyHandler传入代理构建一个handler</span></span><br><span class="line">handler = request.ProxyHandler(&#123;<span class="string">'http'</span>: <span class="string">'59.44.78.30:54069'</span>&#125;)</span><br><span class="line"><span class="comment"># 2. 构建opener</span></span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line"><span class="comment"># 3. 使用opener发送请求</span></span><br><span class="line">resp = opener.open(url)</span><br><span class="line">print(resp.read)</span><br></pre></td></tr></table></figure><p>可以看到输出的ip地址已经发生改变，但是免费的代理很慢，所以建议花点小钱买个代理。</p><h2 id="Cookie"><a class="header-anchor" href="#Cookie"></a>Cookie</h2><p>在需要登录的网站进行访问时，比如我们访问某一个人的主页，那么如果你没有登录一般是看不了的，它会重定向到登录界面。当我们登录之后就可以进行访问。但是我们是用爬虫代码因此不可能进行登录。因袭我们只要手动进行登录进入某人的主页，并获取到改主页的<code>cookie</code>信息，将<code>cookie</code>信息添加到请求头中即可进行访问。</p><h2 id="http-cookiejar模块"><a class="header-anchor" href="#http-cookiejar模块"></a>http.cookiejar模块</h2><p>有没有一种方法不需要我们手动获取<code>cookie</code>信息，显得太low，我们这里使用一种方式全自动的获取某人主页。</p><p>该模块主要的类有以下4种:</p><ul><li><code>CookieJar</code>:管理HTTP <code>cookie</code>值，存储HTTP请求生成的<code>cookie</code>,向传出的HTTP请求添加<code>cookie</code>的对象。整个<code>cookie</code>都存储在内存中，对<code>CookieJar</code>实例进行垃圾回收后<code>cookie</code>也将丢失；</li><li><code>FileCookieJar(filename, delayload=None, policy=None)</code>:从<code>CookieJar</code>派生而来，检索<code>cookie</code>信息并将<code>cookie</code>存储到文件中。<code>filename</code>是存储<code>cookie</code>的文件名，<code>delayload</code>为<code>True</code>时支持延迟访问文件，即只在有需要时才读取文件或在文件中存储数据。</li><li><code>MozillaCookieJar</code>:从<code>FileCookieJar</code>派生而来，创建与Mozilla浏览器<code>cookies.txt</code>兼容的<code>FileCookieJar</code>实例</li><li><code>LWPCookieJar</code>:从<code>FileCookieJar</code>派生而来，创建与<code>libwww-per</code>标准的<code>Set-Cookie3</code>文件格式兼容的<code>FileCookieJar</code>实例。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"><span class="keyword">from</span> http.cookiejar <span class="keyword">import</span> CookieJar</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">''</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span> <span class="title">_opener</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    登录准备</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 创建一个cookiejar对象</span></span><br><span class="line">    cookiejar = CookieJar()</span><br><span class="line">    <span class="comment"># 使用cookiejar创建一个HTTPCookieProcess对象</span></span><br><span class="line">    handler = request.HTTPCookieProcessor(cookiejar)</span><br><span class="line">    <span class="comment"># 使用handler创建一个opener</span></span><br><span class="line">    opener = request.build_opener(handler)</span><br><span class="line">    <span class="keyword">return</span> opener</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(opener)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    登录</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 填入自己的账号密码</span></span><br><span class="line">    data = &#123;<span class="string">'email'</span>: <span class="string">''</span>,</span><br><span class="line">           <span class="string">'password'</span>: <span class="string">''</span>&#125;</span><br><span class="line">    login_url = <span class="string">'http://www.renren.com/PLogin.do'</span></span><br><span class="line">    req = request.Request(login_url, </span><br><span class="line">                          data=parse.urlencode(data).encode(<span class="string">'utf-8'</span>), </span><br><span class="line">                          headers=headers)</span><br><span class="line">    opener.open(req)</span><br><span class="line">    </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">visit</span><span class="params">(opener)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    访问个人主页</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    url = <span class="string">'http://www.renren.com/880151247/profile'</span></span><br><span class="line">    req = request.Request(url, headers=headers)</span><br><span class="line">    resp = opener.open(req)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'renren.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(resp.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    opener = get_opener()</span><br><span class="line">    login(opener)</span><br><span class="line">    visit(opener)</span><br></pre></td></tr></table></figure><h2 id="保存cookie信息到本地"><a class="header-anchor" href="#保存cookie信息到本地"></a>保存cookie信息到本地</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> http.cookiejar <span class="keyword">import</span> MozillaCookieJar</span><br><span class="line"></span><br><span class="line">cookiejar = MozillaCookieJar(<span class="string">'cookie.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cookiejar.load(ignore_discard=True)</span></span><br><span class="line">handler = request.HTTPCookieProcessor(cookiejar)</span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line">opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="comment"># 有的cookie信息会在代码结束后过期，因此添加参数ignore_discard保存所有的cookie</span></span><br><span class="line">cookiejar.save(ignore_discard=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="读取保存在本地的cookie信息"><a class="header-anchor" href="#读取保存在本地的cookie信息"></a>读取保存在本地的cookie信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> http.cookiejar <span class="keyword">import</span> MozillaCookieJar</span><br><span class="line"></span><br><span class="line">cookiejar = MozillaCookieJar(<span class="string">'cookie.txt'</span>)</span><br><span class="line">cookiejar.load(ignore_discard=<span class="literal">True</span>)</span><br><span class="line">handler = request.HTTPCookieProcessor(cookiejar)</span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line">opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="comment"># 有的cookie信息会在代码结束后过期，因此添加参数ignore_discard保存所有的cookie</span></span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookiejar:</span><br><span class="line">    print(cookie)</span><br></pre></td></tr></table></figure><h1>参考</h1><ul><li><a href="https://www.bilibili.com/video/BV1Lx41197NM?p=1" target="_blank" rel="noopener">python爬虫_从入门到精通（基础篇）http协议、requests库、beautifulsoup库、正则表达式、存取、lxml、xpath、css选择器</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo + Yilia + Github Pages 博客搭建</title>
      <link href="/blog_building.html"/>
      <url>/blog_building.html</url>
      
        <content type="html"><![CDATA[<p>作为一个互联网技术人员，拥有一个自己的博客是必不可少的。可以记录自己平时的感悟，结交各方英雄豪杰。话不多说，直接干货。</p><a id="more"></a><h2 id="开发环境："><a class="header-anchor" href="#开发环境："></a>开发环境：</h2><p>在win10的Microsoft Store中安装Ubuntu18.04。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/ApplicationFrameHost_ZMauW7GyZi.png?x-oss-process=style/ossProcess" alt=""></p><h2 id="博客框架"><a class="header-anchor" href="#博客框架"></a>博客框架</h2><p>搭建一个博客既可以自己前端后端都搞定，从头到尾写一遍，也可以使用一些目前已有的框架进行快速搭建。博客框架有动态和静态之分，动态的意思就是有后端，比如说有登录功能；静态就是一个纯前端的直接进行展示的框架。</p><p>动态框架有：</p><ul><li><a href="https://wordpress.org" target="_blank" rel="noopener">WordPress</a></li></ul><p>静态框架有：</p><ul><li><a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a></li><li><a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a></li><li><a href="http://www.gohugo.org" target="_blank" rel="noopener">Hugo</a></li><li><a href="https://vuepress.vuejs.org" target="_blank" rel="noopener">VuePress</a></li><li><a href="https://solo.b3log.org" target="_blank" rel="noopener">Solo</a></li></ul><p>我们这里使用Hexo进行博客搭建，Hexo是一款基于Node.js的静态博客框架，使用Markdown解析文章。</p><ol><li><p>安装Node.js</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -sL https://deb.nodesource.com/setup_13.x | sudo -E bash -</span><br><span class="line">sudo apt-get install -y nodejs</span><br></pre></td></tr></table></figure></li><li><p>安装Hexo</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure></li><li><p>新建博客</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir temp</span><br><span class="line">cd temp</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure></li></ol><p>至此一个最简单的博客便搭建好了，博客目录下的文件是这样的，<strong>以后的shell操作目录都是在temp下</strong>：</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/ubuntu1804_aCMDn8m8xg.png?x-oss-process=style/ossProcess" alt=""></p><p>可以使用<code>hexo s</code>进行访问，打开浏览器输入<code>localhost:4000</code>即可访问</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_7HxBQLGoDh.png?x-oss-process=style/ossProcess" alt=""></p><h2 id="主题"><a class="header-anchor" href="#主题"></a>主题</h2><p>现在我们已经搭建好了自己的博客，现在我们对他进行自定义修改，替换掉它的默认主题，换成目前比较流行的主题<a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="noopener">Yilia</a>,当然也可以去挑选其他<a href="https://hexo.io/themes/" target="_blank" rel="noopener">主题</a></p><ol><li>首先获取Yilia主题，并将它放置在<code>themes</code>文件夹下</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/litten/hexo-theme-yilia.git ./themes/yilia</span><br></pre></td></tr></table></figure><ol start="2"><li><p>对博客配置文件<code>_config.yml</code>进行修改，将<code>theme: landscape</code>修改为<code>theme: yilia</code>，记住，冒号后需跟一空格</p></li><li><p>然后进行编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo cl</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>打开<code>localhost:4000</code></p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_JOw3vpX6iX.png?x-oss-process=style/ossProcess" alt=""></p></li></ol><h2 id="外网访问"><a class="header-anchor" href="#外网访问"></a>外网访问</h2><p>目前我们的博客还只能在本地进行查看，如何将博客部署到远端，一个方法是自己搭个服务器，或者买个云服务器，这个花钱的我们以后介绍。下面介绍一种免费的方式—Github Pages.</p><ol><li><p>创建Github Repository</p><p>首先需要登录自己的Github账号，并新建一个repository，记住<code>Repository name</code>必须填为<code>Owner.github.io</code>,然后点创建即可。</p></li></ol><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_8262XSmhKU.png?x-oss-process=style/ossProcess" alt=""></p><ol start="2"><li><p>配置本地文件到Github</p><p>首先安装插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>然后编辑<code>_config.yaml</code>,修改<code>deploy</code>为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: https://github.com/zengruizhao/zengruizhao.github.io.git# 这里进行替换</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></li><li><p>发布博客</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo cl</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>这边会让我们输入Github的账号密码，为了防止以后每次<code>hexo d</code>都要输入密码，这边进行设置，这样只要第一次输入账号密码即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global credential.helper store</span><br></pre></td></tr></table></figure></li></ol><p>那么现在你可以直接打开网址<code>Owner.github.io</code>就可以进行查看了。</p><h2 id="绑定个人域名"><a class="header-anchor" href="#绑定个人域名"></a>绑定个人域名</h2><p>怎么，嫌这个网址太长？没关系，我们可以绑定我们自己的域名，只要花点钱去买个域名就行，我在<a href="https://dnspod.cloud.tencent.com/" target="_blank" rel="noopener">腾讯云</a>购买了一个域名<code>darlewo.cn</code></p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_f3UZM3557P.png?x-oss-process=style/ossProcess" alt=""></p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_J8yMMKv35S.png?x-oss-process=style/ossProcess" alt=""></p><p>进入域名管理-解析,按照如上图所示进行添加记录，其中记录值填为你的<code>Owner.github.io</code>的IP，IP的获取你可以在Windows PowerShell 中ping一下你的<code>Owner.github.io</code>.</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_x2RCm1kCdx.png?x-oss-process=style/ossProcess" alt=""></p><p>然后进入你的Github repository 中新建一个文件CNAME,并在里面填入你申请的域名即可。</p><p><img src="https://darlewo.oss-cn-beijing.aliyuncs.com/ossImgs/msedge_SBZtnBGwf7.png?x-oss-process=style/ossProcess" alt=""></p><p>然后你就可以直接输入你自己的域名进行访问了！</p><h2 id="结尾"><a class="header-anchor" href="#结尾"></a>结尾</h2><p>第一篇博客就是想简单的记录一下自己搭建博客的一个过程。当然这只是第一步，之后会继续更新博客的个性化制作，通过一些插件来更好的完善我们的博客。</p><p>本人虽然不是博客新手，之前一直都在<a href="https://blog.csdn.net/z13653662052" target="_blank" rel="noopener">CSDN</a>撰写博客，但也是第一次搭建自己的博客，经过自己的摸索，朋友<a href="http://cyyan.cn" target="_blank" rel="noopener">朝阳</a>和<a href="http://jwxie.cn" target="_blank" rel="noopener">嘉伟</a>的帮助，成功的将自己的博客搭建起来，日后会好好进行打理，如果您有什么问题或建议，请各位好友提出您宝贵的意见🤝。</p><h2 id="参考资料"><a class="header-anchor" href="#参考资料"></a>参考资料</h2><ul><li><a href="http://www.xuyankun.cn/2017/05/06/pages/" target="_blank" rel="noopener">如何用Github pages和Hexo搭建个人博客</a></li><li><a href="https://www.bilibili.com/video/BV1Yb411a7ty" target="_blank" rel="noopener">手把手教你从0开始搭建自己的个人博客 |无坑版视频教程| hexo</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Yilia </tag>
            
            <tag> Github Pages </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
